{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from constants.paths import INTERMEDIATE_PC_PRICE_DIR, INTERMEDIATE_PHENOL_ACETONE_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Analysing intermediate data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_price_eu = pd.read_csv(INTERMEDIATE_PC_PRICE_DIR / \"intermediate_pc_price_eu.csv\")\n",
    "pc_price_asia = pd.read_csv(\n",
    "    INTERMEDIATE_PC_PRICE_DIR / \"intermediate_pc_price_asia.csv\"\n",
    ")\n",
    "bpa_capacity_loss = pd.read_csv(\n",
    "    INTERMEDIATE_PHENOL_ACETONE_DIR / \"intermediate_bpa_capacity_loss.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpa_capacity_loss[\"date\"] = pd.to_datetime(\n",
    "    bpa_capacity_loss[\"date\"], format=\"%Y-%m-%d\"\n",
    ").dt.to_period(\"M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# TODO - Select features for modelling\n",
    "\n",
    "- Correlation analysis\n",
    "- ADF testing (stationarity of features)\n",
    "- Granger causality tests (past values of features helping predict target)\n",
    "- Time-lagged mutual information\n",
    "- Independence testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling_features(\n",
    "    df: pd.DataFrame, window_sizes: list[int], target_cols: list[str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Add rolling mean and std features for specified window sizes.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe.\n",
    "        window_sizes (list[int]): List of window sizes for rolling calculations.\n",
    "        target_cols (list[str]): Column names to compute rolling features on.\n",
    "\n",
    "        Returns: pd.DataFrame with new rolling features added.\n",
    "    \"\"\"\n",
    "    for target_col in target_cols:\n",
    "        for window in window_sizes:\n",
    "            df[f\"{target_col}_roll_mean_{window}\"] = (\n",
    "                df[target_col].rolling(window=window).mean()\n",
    "            )\n",
    "            df[f\"{target_col}_roll_std_{window}\"] = (\n",
    "                df[target_col].rolling(window=window).std()\n",
    "            )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rate_of_change_features(\n",
    "    df: pd.DataFrame, periods: list[int], target_cols: list[str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Add rate of change features for specified periods.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe.\n",
    "        periods (list[int]): List of periods for rate of change calculations.\n",
    "        target_cols (list[str]): Column names to compute rate of change features on.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame with new rate of change features added.\n",
    "    \"\"\"\n",
    "    new_cols = {}\n",
    "    for target_col in target_cols:\n",
    "        for period in periods:\n",
    "            new_cols[f\"{target_col}_roc_{period}\"] = df[target_col].pct_change(\n",
    "                periods=period, fill_method=None\n",
    "            )\n",
    "    return pd.concat([df, pd.DataFrame(new_cols)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# Analysis Summary\n",
    "\n",
    "## What the Expert Found (Critical Issues with Past Approaches):\n",
    "\n",
    "Major Problems:\n",
    "1. Look-ahead bias - Both groups likely selected features on full datasets, inflating performance\n",
    "2. Single-horizon selection - Used same features for 3-month and 9-month forecasts (suboptimal)\n",
    "3. Exogenous availability problem - Didn't properly account for unavailable future features\n",
    "4. Lag feature explosion - Created 500+ correlated features, risking overfitting\n",
    "5. PCA misuse - Group 2 used PCA on tree models (loses interpretability, minimal benefit)\n",
    "6. Granger causality overuse - Assumes linearity, sensitive to lag choice\n",
    "\n",
    "## What's Good About Their Approaches:\n",
    "\n",
    "Group 1 Strengths:\n",
    "- Rigorous statistical testing (Granger + Mutual Information)\n",
    "- Feature independence testing\n",
    "- Lean final selection (5 features)\n",
    "\n",
    "Group 2 Strengths:\n",
    "- Multi-method ensemble approach\n",
    "- Domain-specific processing (separate virgin/recycled models)\n",
    "- SHAP for explainability\n",
    "\n",
    "## What's Missing:\n",
    "\n",
    "1. Horizon-specific feature sets - Should select different features for 3, 6, 9-month forecasts\n",
    "2. Proper temporal validation - Feature selection should happen within TimeSeriesSplit folds\n",
    "3. Smart lag engineering - Rolling aggregates and rate-of-change instead of raw lags\n",
    "4. Availability constraints - Must ensure lag ≥ forecast horizon\n",
    "5. Domain features - Price spreads, ratios, shutdown intensity metrics\n",
    "\n",
    "# Recommended Approach for Your Project:\n",
    "\n",
    "## Phase 1: Feature Engineering (Better than just lags)\n",
    "\n",
    "Instead of `lag_1`, `lag_2`, `lag_3`, ..., `lag_24`:\n",
    "\n",
    "- Rolling aggregates (smoother): `pc_price_ma3`, `pc_price_ma6`, `pc_price_ma12`\n",
    "\n",
    "- Rate of change (momentum): `pc_price_roc3` = `pc_price.pct_change(3)`\n",
    "\n",
    "- Domain features: `asia_europe_spread` = `pc_asia` - `pc_eu` & `capacity_loss_6m` = `bpa_capacity.rolling(6).sum()`\n",
    "- Temporal (seasonality): `month_sin`, `month_cos`, `quarter`\n",
    "\n",
    "## Phase 2: Horizon-Specific Selection\n",
    "\n",
    "- 3-month forecast: Use lags ≥3, short-term indicators (`ma3`, `roc1`)\n",
    "- 6-month forecast: Use lags ≥6, medium-term trends (`ma6`, `roc3`)\n",
    "- 9-month forecast: Use lags ≥9, long-term fundamentals (`ma12`, `seasonal`)\n",
    "\n",
    "## Phase 3: Selection Methods (Simplified Pipeline)\n",
    "\n",
    "1. Variance threshold (remove constants)\n",
    "2. Correlation with target (|r| > 0.3)\n",
    "3. Random Forest importance (top 50%)\n",
    "4. Remove multicollinearity (correlation > 0.9)\n",
    "5. Optional: Granger causality (for interpretation only)\n",
    "\n",
    "## Phase 4: Validation Strategy\n",
    "\n",
    "- Use TimeSeriesSplit with 5 folds\n",
    "- Select features that appear in ≥60% of folds (robustness)\n",
    "- Never select on full dataset\n",
    "\n",
    "# Key Recommendations:\n",
    "\n",
    "✅ DO:\n",
    "\n",
    "- Create horizon-specific datasets (`features_3m.csv`, `features_6m.csv`, `features_9m.csv`)\n",
    "- Use rolling aggregates instead of raw lags\n",
    "- Ensure lag ≥ forecast horizon\n",
    "- Select features within CV folds (avoid leakage)\n",
    "- Track experiments in MLflow\n",
    "- Use DVC pipeline for reproducibility\n",
    "\n",
    "❌ DON'T:\n",
    "\n",
    "- Create hundreds of lag features\n",
    "- Use PCA for XGBoost/Random Forest\n",
    "- Select same features for all horizons\n",
    "- Include features unavailable at prediction time\n",
    "- Rely solely on Granger causality\n",
    "\n",
    "Priority Actions:\n",
    "\n",
    "Week 1:\n",
    "1. Create src/data_pipelines/intermediate_to_processed.py\n",
    "2. Generate rolling aggregates, rate-of-change, domain features\n",
    "3. Create horizon-specific datasets\n",
    "\n",
    "Week 2:\n",
    "4. Implement feature selection with TimeSeriesSplit\n",
    "5. Run for h=3, 6, 9 separately\n",
    "6. Log to MLflow\n",
    "\n",
    "Expected Results:\n",
    "- 3-month: ~15-25 features\n",
    "- 6-month: ~10-20 features\n",
    "- 9-month: ~8-15 features\n",
    "- MAPE improvement: ~30-50% better than naive baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "How Fold-Based Feature Selection Works\n",
    "\n",
    "Step-by-Step Mechanism:\n",
    "\n",
    "# Conceptual flow:\n",
    "For each of 5 CV folds:\n",
    "- 1. Split data: Train (expanding) → Test\n",
    "- 2. Run feature selection on TRAIN only\n",
    "- 3. Record which features were selected\n",
    "\n",
    "After all folds:\n",
    "- 4. Count how many times each feature was selected\n",
    "- 5. Keep features that appeared in ≥ threshold × folds\n",
    "\n",
    "Your Concrete Example Analyzed:\n",
    "\n",
    "Given your example where each fold selected 5 features:\n",
    "- Fold 1: [A, B, C, D, E]\n",
    "- Fold 2: [A, B, D, F, G]\n",
    "- Fold 3: [A, C, D, F, H]\n",
    "- Fold 4: [B, C, D, E, F]\n",
    "- Fold 5: [A, D, F, G, I]\n",
    "\n",
    "Feature counts:\n",
    "- D: 5/5 folds (100%) → Must include (stable across all time periods)\n",
    "- A, F: 4/5 folds (80%) → Very strong candidates\n",
    "- B, C: 3/5 folds (60%) → Borderline at 60% threshold\n",
    "- E, G: 2/5 folds (40%) → Reject at 60% threshold\n",
    "- H, I: 1/5 fold (20%) → Definitely reject\n",
    "\n",
    "At 60% threshold (3/5 folds): Select [A, B, C, D, F] (5 features)\n",
    "At 80% threshold (4/5 folds): Select [A, D, F] (3 features)\n",
    "\n",
    "Why This Is Better Than Selecting on Full Training Set:\n",
    "\n",
    "Problem it solves:\n",
    "1. Spurious correlations - If feature X is only useful in 2019-2020 but useless in 2021-2023, full-set selection might keep it. Fold-based\n",
    "approach reveals it's unstable.\n",
    "2. Overfitting to specific time periods - A feature that looks great on full training data might only work in one market regime.\n",
    "3. Data leakage detection - If a feature is selected in early folds but not later folds, it might be \"looking ahead\" somehow.\n",
    "\n",
    "Concrete example:\n",
    "- Feature: covid_impact (dummy for 2020-2021)\n",
    "- Full-set selection: High importance (captures 2020 price drop)\n",
    "- Fold-based: Only selected in folds that include 2020 → Low consistency → Rejected\n",
    "- Result: More generalizable model\n",
    "\n",
    "Threshold Choice:\n",
    "\n",
    "| Threshold  | Meaning                             | When to Use                                          |\n",
    "|------------|-------------------------------------|------------------------------------------------------|\n",
    "| 100% (5/5) | Feature must be useful in ALL folds | Very conservative, very few features, might underfit |\n",
    "| 80% (4/5)  | Strong consensus                    | Good for production models, high confidence          |\n",
    "| 60% (3/5)  | Majority vote                       | Default recommendation - balanced                    |\n",
    "| 40% (2/5)  | Lenient                             | Small datasets, exploratory phase                    |\n",
    "| 20% (1/5)  | Very lenient                        | Too permissive, defeats the purpose                  |\n",
    "\n",
    "For your PC price case (150 observations):\n",
    "- Recommended: 40% (2/5 folds)\n",
    "- Reason: Small dataset means each fold has ~30 test samples, higher variance in selection\n",
    "- Alternative: Use 3-fold CV with 60% threshold (2/3 folds) → larger folds, same logic\n",
    "\n",
    "Key Adjustments for Your Small Dataset:\n",
    "\n",
    "The expert recommended several modifications:\n",
    "\n",
    "1. Lower threshold to 40% (2/5 folds) instead of 60%\n",
    "- Reason: With only 150 observations, feature selection is noisier\n",
    "2. Add 3-month embargo between train/test\n",
    "tscv = TimeSeriesSplit(n_splits=5, gap=3)  # 3-month gap\n",
    "- Prevents label leakage from autocorrelation\n",
    "3. Consider 3-fold CV instead of 5-fold\n",
    "- Larger folds → more stable selection\n",
    "- 60% threshold = 2/3 folds (same as 40% with 5-fold)\n",
    "4. Use feature groups to ensure diversity\n",
    "FEATURE_GROUPS = {\n",
    "    'autoregressive': ['pc_price_lag3', 'pc_price_lag6', ...],\n",
    "    'commodities': ['oil_price_lag6', 'gas_price_lag6', ...],\n",
    "    'supply': ['capacity_loss_6m', 'shutdown_count_6m', ...],\n",
    "    'temporal': ['month_sin', 'month_cos', 'quarter']\n",
    "}\n",
    "# Ensure at least 1 feature from each group is selected\n",
    "\n",
    "Implementation Approach:\n",
    "\n",
    "The expert provided code showing you should:\n",
    "\n",
    "1. Test multiple thresholds as a hyperparameter:\n",
    "```python\n",
    "for threshold in [0.2, 0.4, 0.6, 0.8]:\n",
    "    features = select_with_threshold(threshold)\n",
    "    model = train_model(features)\n",
    "    performance = evaluate(model)\n",
    "    mlflow.log_metrics({\"rmse\": performance, \"threshold\": threshold})\n",
    "```\n",
    "2. Track everything in MLflow:\n",
    "- Consensus threshold (e.g., 0.4)\n",
    "- Feature counts per fold\n",
    "- Importance scores averaged across folds\n",
    "- Final feature list\n",
    "3. Validate the threshold choice:\n",
    "- Plot: threshold (x-axis) vs. RMSE (y-axis)\n",
    "- Look for \"elbow\" where performance plateaus\n",
    "- Choose threshold that balances features vs. performance\n",
    "\n",
    "Bottom Line:\n",
    "\n",
    "For your PC price forecasting project with 150 monthly observations:\n",
    "\n",
    "Recommended Configuration:\n",
    "CV_CONFIG = {\n",
    "    'n_splits': 5,           # Standard\n",
    "    'gap': 3,                # 3-month embargo\n",
    "    'threshold': 0.4,        # 2/5 folds (lenient for small data)\n",
    "    'max_train_size': None   # Use all available history\n",
    "}\n",
    "\n",
    "Expected outcome:\n",
    "- ~10-15 features selected at 40% threshold\n",
    "- More robust than single-train selection\n",
    "- Computational cost: 5x (acceptable for your dataset size)\n",
    "- Interpretability: Can explain \"feature X was useful in 4/5 time periods\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Use a simple, fast model for feature selection, then use those features with your actual modeling approaches (XGBoost,\n",
    "Prophet, etc.).\n",
    "\n",
    "Two-Stage Process:\n",
    "\n",
    "# STAGE 1: Feature Selection (uses simple model)\n",
    "Goal: Reduce from 100+ features to ~15-20 robust features\n",
    "\n",
    "```python\n",
    "selected_features = select_features_with_cv(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    selector_model=RandomForestRegressor(  # <-- Simple model for selection\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "# STAGE 2: Actual Modeling (uses selected features)\n",
    "Goal: Find best model for prediction\n",
    "\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "# Try different models with the SAME feature set:\n",
    "```python\n",
    "models = {\n",
    "    'xgboost': XGBRegressor(...),\n",
    "    'random_forest': RandomForestRegressor(...),\n",
    "    'prophet': Prophet(...),\n",
    "    'sarima': SARIMAX(...)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    predictions = model.predict(X_test_selected)\n",
    "    # Compare performance...\n",
    "```\n",
    "\n",
    "Why This Works:\n",
    "\n",
    "1. Feature selection is model-agnostic (mostly)\n",
    "- Features that help Random Forest usually help XGBoost too\n",
    "- Both are tree-based and capture similar relationships\n",
    "- Good features for ML models often good for time-series models too\n",
    "2. Selection model should be:\n",
    "- Fast (you're running 5-fold CV)\n",
    "- Robust (not too prone to overfitting)\n",
    "- Similar to final models (if possible)\n",
    "\n",
    "Recommended Selection Models:\n",
    "\n",
    "| Your Final Models                | Use This For Selection                                | Why                                         |\n",
    "|----------------------------------|-------------------------------------------------------|---------------------------------------------|\n",
    "| XGBoost, CatBoost, Random Forest | RandomForestRegressor(n_estimators=100, max_depth=10) | Fast, similar algorithm family, robust      |\n",
    "| Linear models (Ridge, Lasso)     | LassoCV or correlation-based                          | Matches model assumptions                   |\n",
    "| Prophet, SARIMA                  | Correlation + domain knowledge                        | Time-series models use features differently |\n",
    "| Mixed (trying everything)        | RandomForestRegressor                                 | Most versatile, works for all               |\n",
    "\n",
    "For Your PC Price Project:\n",
    "\n",
    "Recommended approach:\n",
    "\n",
    "## Stage 1: Feature Selection\n",
    "Use Random Forest because you'll try multiple model types\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "def select_features_cv(X, y, n_splits=5, threshold=0.4):\n",
    "    \"\"\"\n",
    "    Select features using Random Forest in CV folds.\n",
    "    \"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits, gap=3)\n",
    "    feature_counts = Counter()\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "\n",
    "        # Simple, fast Random Forest for selection\n",
    "        rf = RandomForestRegressor(\n",
    "            n_estimators=100,      # Enough for stable importance\n",
    "            max_depth=10,          # Prevent overfitting\n",
    "            min_samples_leaf=5,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "        # Keep top 50% by importance\n",
    "        threshold_imp = np.percentile(rf.feature_importances_, 50)\n",
    "        selected = X_train.columns[rf.feature_importances_ > threshold_imp]\n",
    "\n",
    "        feature_counts.update(selected)\n",
    "\n",
    "    # Return features appearing in ≥40% of folds\n",
    "    min_appearances = int(threshold * n_splits)  # 2/5 folds\n",
    "    robust_features = [\n",
    "        feat for feat, count in feature_counts.items()\n",
    "        if count >= min_appearances\n",
    "    ]\n",
    "\n",
    "    return robust_features\n",
    "\n",
    "# Use it:\n",
    "selected_features = select_features_cv(X_train, y_train)\n",
    "\n",
    "print(f\"Selected {len(selected_features)} features from {len(X_train.columns)}\")\n",
    "```\n",
    "\n",
    "Then:\n",
    "\n",
    "## Stage 2: Model Comparison\n",
    "Try different models with the SAME selected features\n",
    "\n",
    "```python\n",
    "X_train_sel = X_train[selected_features]\n",
    "X_test_sel = X_test[selected_features]\n",
    "\n",
    "models_to_compare = {\n",
    "    'xgboost': XGBRegressor(n_estimators=1000, learning_rate=0.01),\n",
    "    'catboost': CatBoostRegressor(iterations=1000, learning_rate=0.05),\n",
    "    'random_forest': RandomForestRegressor(n_estimators=500),\n",
    "    # Prophet and SARIMA handle features differently, see below\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models_to_compare.items():\n",
    "    model.fit(X_train_sel, y_train)\n",
    "    preds = model.predict(X_test_sel)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "    results[name] = rmse\n",
    "\n",
    "    # Log to MLflow\n",
    "    with mlflow.start_run(run_name=f\"{name}_h3\"):\n",
    "        mlflow.log_param(\"model_type\", name)\n",
    "        mlflow.log_param(\"n_features\", len(selected_features))\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "```\n",
    "\n",
    "### Special Case: Time-Series Models (Prophet, SARIMA)\n",
    "\n",
    "Important: Prophet and SARIMA use exogenous features differently:\n",
    "\n",
    "#### For Prophet:\n",
    "Only use features that make sense as regressors\n",
    "\n",
    "```python\n",
    "prophet_features = [f for f in selected_features\n",
    "                    if 'lag' not in f]  # Prophet handles lags internally\n",
    "\n",
    "model = Prophet()\n",
    "for feat in prophet_features:\n",
    "    model.add_regressor(feat)\n",
    "\n",
    "# Prepare data in Prophet format\n",
    "prophet_df = pd.DataFrame({\n",
    "    'ds': train_dates,\n",
    "    'y': y_train,\n",
    "    **{feat: X_train_sel[feat] for feat in prophet_features}\n",
    "})\n",
    "model.fit(prophet_df)\n",
    "```\n",
    "\n",
    "#### For SARIMA:\n",
    "Use selected features as exogenous variables\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "model = SARIMAX(\n",
    "    y_train,\n",
    "    exog=X_train_sel[selected_features],  # Use all selected features\n",
    "    order=(1, 1, 1),\n",
    "    seasonal_order=(1, 1, 1, 12)\n",
    ")\n",
    "results = model.fit()\n",
    "```\n",
    "\n",
    "Alternative: Model-Specific Selection\n",
    "\n",
    "If you want to be more rigorous, you could select features separately for each model type:\n",
    "\n",
    "- Option 1: One feature set for all models (simpler)\n",
    "features_all = select_features_cv(X, y, selector_model=RandomForestRegressor())\n",
    "\n",
    "- Option 2: Model-specific feature sets (more complex)\n",
    "features_for_xgboost = select_features_cv(X, y, selector_model=XGBRegressor())\n",
    "features_for_linear = select_features_cv(X, y, selector_model=LassoCV())\n",
    "features_for_ts = select_features_correlation(X, y)  # Correlation-based\n",
    "\n",
    "Then train each model with its own features\n",
    "\n",
    "Recommendation: Start with Option 1 (single feature set using Random Forest). Only use Option 2 if you find significant performance\n",
    "differences.\n",
    "\n",
    "Summary:\n",
    "\n",
    "- Feature selection model = Simple, fast model to identify important features (Random Forest)\n",
    "- Final prediction models = The actual models you'll compare (XGBoost, CatBoost, Prophet, SARIMA, etc.)\n",
    "\n",
    "Think of it as:\n",
    "- Feature selection = \"Which ingredients are important?\" (use a simple recipe)\n",
    "- Final modeling = \"What's the best way to cook those ingredients?\" (try multiple recipes)\n",
    "\n",
    "The feature selection step just reduces dimensionality and removes noise. The final modeling step is where you actually optimize for\n",
    "prediction performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "14-data-challenge (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
