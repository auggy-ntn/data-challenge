{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from constants.paths import INTERMEDIATE_PC_PRICE_DIR, INTERMEDIATE_PHENOL_ACETONE_DIR\n",
    "from src.data_pipelines.uni_intermediate_to_processed import build_univariate_dataset\n",
    "import src.utils.feature_engineering as fe_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Analysing intermediate data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_price_eu = pd.read_csv(INTERMEDIATE_PC_PRICE_DIR / \"intermediate_pc_price_eu.csv\")\n",
    "pc_price_asia = pd.read_csv(\n",
    "    INTERMEDIATE_PC_PRICE_DIR / \"intermediate_pc_price_asia.csv\"\n",
    ")\n",
    "bpa_capacity_loss = pd.read_csv(\n",
    "    INTERMEDIATE_PHENOL_ACETONE_DIR / \"intermediate_bpa_capacity_loss.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Feature Engineering approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "There are 2 approaches to feature engineering depending on the modeling technique we want to use:\n",
    "1. PC type specific models: Use a wide format dataset (1 column per PC type).\n",
    "2. Global multivariate model: Use a long format dataset (1 row per PC type per country per month) with all features included."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 1 - PC type specific models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "In this approach, we will create a wide format dataset where each PC type has its own column. Features will be engineered specifically for each PC type. We first create a large dataset with all possible features for each PC type, and select a subset of that dataset at modeling time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "The base dataset is created using the `build_univariate_dataset` function in `src/data_pipelines/uni_intermediate_to_processed.py`. This function simply concatenates all datasets (PC prices and exogenous variables) into a wide format dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_utils.create_wide_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "The `asia_pc_gf_best_price` column is weird. Possible that values within the column don't have the same unit. Comes from data, nothing we can really do about it..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "This is the base for the pipeline that creates the final processed dataset used for modeling in `src/data_pipelines/uni_intermediate_to_processed.py`. The final dataset is built by adding features to the base dataset using functions from `src/utils/feature_engineering.py`.\n",
    "The feature engineering steps are as follows:\n",
    "- Calendar features (month, quarter, year) with cyclical encoding for month and quarter.\n",
    "- Lag features for PC prices.\n",
    "- Rolling window statistics for PC prices.\n",
    "- Rate of change features for PC prices.\n",
    "- Lag features for exogenous variables.\n",
    "- Rolling window statistics for exogenous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_univariate_dataset(horizon=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Once the base wide format dataset is created, we can engineer potential features (which will be filtered later). See the file `src/utils/feature_engineering.py` for the implementation of the feature engineering functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 2 - Global multivariate model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "In this approach , we will create a long format dataset where each row corresponds to a specific PC type in a specific region at a specific month."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "```\n",
    "| date       | region | pc_type | price | ... features ... |\n",
    "|------------|--------|---------|-------|------------------|\n",
    "| 2020-01-01 | europe | crystal | 2.5   | ...              |\n",
    "| 2020-01-01 | europe | gf10    | 2.8   | ...              |\n",
    "| 2020-01-01 | asia   | gp      | 2.3   | ...              |\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_utils.create_long_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "# TODO - Select features for modelling\n",
    "\n",
    "- Correlation analysis\n",
    "- ADF testing (stationarity of features)\n",
    "- Granger causality tests (past values of features helping predict target)\n",
    "- Time-lagged mutual information\n",
    "- Independence testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "# Analysis Summary\n",
    "\n",
    "## What the Expert Found (Critical Issues with Past Approaches):\n",
    "\n",
    "Major Problems:\n",
    "1. Look-ahead bias - Both groups likely selected features on full datasets, inflating performance\n",
    "2. Single-horizon selection - Used same features for 3-month and 9-month forecasts (suboptimal)\n",
    "3. Exogenous availability problem - Didn't properly account for unavailable future features\n",
    "4. Lag feature explosion - Created 500+ correlated features, risking overfitting\n",
    "5. PCA misuse - Group 2 used PCA on tree models (loses interpretability, minimal benefit)\n",
    "6. Granger causality overuse - Assumes linearity, sensitive to lag choice\n",
    "\n",
    "## What's Good About Their Approaches:\n",
    "\n",
    "Group 1 Strengths:\n",
    "- Rigorous statistical testing (Granger + Mutual Information)\n",
    "- Feature independence testing\n",
    "- Lean final selection (5 features)\n",
    "\n",
    "Group 2 Strengths:\n",
    "- Multi-method ensemble approach\n",
    "- Domain-specific processing (separate virgin/recycled models)\n",
    "- SHAP for explainability\n",
    "\n",
    "## What's Missing:\n",
    "\n",
    "1. Horizon-specific feature sets - Should select different features for 3, 6, 9-month forecasts\n",
    "2. Proper temporal validation - Feature selection should happen within TimeSeriesSplit folds\n",
    "3. Smart lag engineering - Rolling aggregates and rate-of-change instead of raw lags\n",
    "4. Availability constraints - Must ensure lag ≥ forecast horizon\n",
    "5. Domain features - Price spreads, ratios, shutdown intensity metrics\n",
    "\n",
    "# Recommended Approach for Your Project:\n",
    "\n",
    "## Phase 1: Feature Engineering (Better than just lags)\n",
    "\n",
    "Instead of `lag_1`, `lag_2`, `lag_3`, ..., `lag_24`:\n",
    "\n",
    "- Rolling aggregates (smoother): `pc_price_ma3`, `pc_price_ma6`, `pc_price_ma12`\n",
    "\n",
    "- Rate of change (momentum): `pc_price_roc3` = `pc_price.pct_change(3)`\n",
    "\n",
    "- Domain features: `asia_europe_spread` = `pc_asia` - `pc_eu` & `capacity_loss_6m` = `bpa_capacity.rolling(6).sum()`\n",
    "- Temporal (seasonality): `month_sin`, `month_cos`, `quarter`\n",
    "\n",
    "## Phase 2: Horizon-Specific Selection\n",
    "\n",
    "- 3-month forecast: Use lags ≥3, short-term indicators (`ma3`, `roc1`)\n",
    "- 6-month forecast: Use lags ≥6, medium-term trends (`ma6`, `roc3`)\n",
    "- 9-month forecast: Use lags ≥9, long-term fundamentals (`ma12`, `seasonal`)\n",
    "\n",
    "## Phase 3: Selection Methods (Simplified Pipeline)\n",
    "\n",
    "1. Variance threshold (remove constants)\n",
    "2. Correlation with target (|r| > 0.3)\n",
    "3. Random Forest importance (top 50%)\n",
    "4. Remove multicollinearity (correlation > 0.9)\n",
    "5. Optional: Granger causality (for interpretation only)\n",
    "\n",
    "## Phase 4: Validation Strategy\n",
    "\n",
    "- Use TimeSeriesSplit with 5 folds\n",
    "- Select features that appear in ≥60% of folds (robustness)\n",
    "- Never select on full dataset\n",
    "\n",
    "# Key Recommendations:\n",
    "\n",
    "✅ DO:\n",
    "\n",
    "- Create horizon-specific datasets (`features_3m.csv`, `features_6m.csv`, `features_9m.csv`)\n",
    "- Use rolling aggregates instead of raw lags\n",
    "- Ensure lag ≥ forecast horizon\n",
    "- Select features within CV folds (avoid leakage)\n",
    "- Track experiments in MLflow\n",
    "- Use DVC pipeline for reproducibility\n",
    "\n",
    "❌ DON'T:\n",
    "\n",
    "- Create hundreds of lag features\n",
    "- Use PCA for XGBoost/Random Forest\n",
    "- Select same features for all horizons\n",
    "- Include features unavailable at prediction time\n",
    "- Rely solely on Granger causality\n",
    "\n",
    "Priority Actions:\n",
    "\n",
    "Week 1:\n",
    "1. Create src/data_pipelines/intermediate_to_processed.py\n",
    "2. Generate rolling aggregates, rate-of-change, domain features\n",
    "3. Create horizon-specific datasets\n",
    "\n",
    "Week 2:\n",
    "4. Implement feature selection with TimeSeriesSplit\n",
    "5. Run for h=3, 6, 9 separately\n",
    "6. Log to MLflow\n",
    "\n",
    "Expected Results:\n",
    "- 3-month: ~15-25 features\n",
    "- 6-month: ~10-20 features\n",
    "- 9-month: ~8-15 features\n",
    "- MAPE improvement: ~30-50% better than naive baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "How Fold-Based Feature Selection Works\n",
    "\n",
    "Step-by-Step Mechanism:\n",
    "\n",
    "# Conceptual flow:\n",
    "For each of 5 CV folds:\n",
    "- 1. Split data: Train (expanding) → Test\n",
    "- 2. Run feature selection on TRAIN only\n",
    "- 3. Record which features were selected\n",
    "\n",
    "After all folds:\n",
    "- 4. Count how many times each feature was selected\n",
    "- 5. Keep features that appeared in ≥ threshold × folds\n",
    "\n",
    "Your Concrete Example Analyzed:\n",
    "\n",
    "Given your example where each fold selected 5 features:\n",
    "- Fold 1: [A, B, C, D, E]\n",
    "- Fold 2: [A, B, D, F, G]\n",
    "- Fold 3: [A, C, D, F, H]\n",
    "- Fold 4: [B, C, D, E, F]\n",
    "- Fold 5: [A, D, F, G, I]\n",
    "\n",
    "Feature counts:\n",
    "- D: 5/5 folds (100%) → Must include (stable across all time periods)\n",
    "- A, F: 4/5 folds (80%) → Very strong candidates\n",
    "- B, C: 3/5 folds (60%) → Borderline at 60% threshold\n",
    "- E, G: 2/5 folds (40%) → Reject at 60% threshold\n",
    "- H, I: 1/5 fold (20%) → Definitely reject\n",
    "\n",
    "At 60% threshold (3/5 folds): Select [A, B, C, D, F] (5 features)\n",
    "At 80% threshold (4/5 folds): Select [A, D, F] (3 features)\n",
    "\n",
    "Why This Is Better Than Selecting on Full Training Set:\n",
    "\n",
    "Problem it solves:\n",
    "1. Spurious correlations - If feature X is only useful in 2019-2020 but useless in 2021-2023, full-set selection might keep it. Fold-based\n",
    "approach reveals it's unstable.\n",
    "2. Overfitting to specific time periods - A feature that looks great on full training data might only work in one market regime.\n",
    "3. Data leakage detection - If a feature is selected in early folds but not later folds, it might be \"looking ahead\" somehow.\n",
    "\n",
    "Concrete example:\n",
    "- Feature: covid_impact (dummy for 2020-2021)\n",
    "- Full-set selection: High importance (captures 2020 price drop)\n",
    "- Fold-based: Only selected in folds that include 2020 → Low consistency → Rejected\n",
    "- Result: More generalizable model\n",
    "\n",
    "Threshold Choice:\n",
    "\n",
    "| Threshold  | Meaning                             | When to Use                                          |\n",
    "|------------|-------------------------------------|------------------------------------------------------|\n",
    "| 100% (5/5) | Feature must be useful in ALL folds | Very conservative, very few features, might underfit |\n",
    "| 80% (4/5)  | Strong consensus                    | Good for production models, high confidence          |\n",
    "| 60% (3/5)  | Majority vote                       | Default recommendation - balanced                    |\n",
    "| 40% (2/5)  | Lenient                             | Small datasets, exploratory phase                    |\n",
    "| 20% (1/5)  | Very lenient                        | Too permissive, defeats the purpose                  |\n",
    "\n",
    "For your PC price case (150 observations):\n",
    "- Recommended: 40% (2/5 folds)\n",
    "- Reason: Small dataset means each fold has ~30 test samples, higher variance in selection\n",
    "- Alternative: Use 3-fold CV with 60% threshold (2/3 folds) → larger folds, same logic\n",
    "\n",
    "Key Adjustments for Your Small Dataset:\n",
    "\n",
    "The expert recommended several modifications:\n",
    "\n",
    "1. Lower threshold to 40% (2/5 folds) instead of 60%\n",
    "- Reason: With only 150 observations, feature selection is noisier\n",
    "2. Add 3-month embargo between train/test\n",
    "tscv = TimeSeriesSplit(n_splits=5, gap=3)  # 3-month gap\n",
    "- Prevents label leakage from autocorrelation\n",
    "3. Consider 3-fold CV instead of 5-fold\n",
    "- Larger folds → more stable selection\n",
    "- 60% threshold = 2/3 folds (same as 40% with 5-fold)\n",
    "4. Use feature groups to ensure diversity\n",
    "FEATURE_GROUPS = {\n",
    "    'autoregressive': ['pc_price_lag3', 'pc_price_lag6', ...],\n",
    "    'commodities': ['oil_price_lag6', 'gas_price_lag6', ...],\n",
    "    'supply': ['capacity_loss_6m', 'shutdown_count_6m', ...],\n",
    "    'temporal': ['month_sin', 'month_cos', 'quarter']\n",
    "}\n",
    "# Ensure at least 1 feature from each group is selected\n",
    "\n",
    "Implementation Approach:\n",
    "\n",
    "The expert provided code showing you should:\n",
    "\n",
    "1. Test multiple thresholds as a hyperparameter:\n",
    "```python\n",
    "for threshold in [0.2, 0.4, 0.6, 0.8]:\n",
    "    features = select_with_threshold(threshold)\n",
    "    model = train_model(features)\n",
    "    performance = evaluate(model)\n",
    "    mlflow.log_metrics({\"rmse\": performance, \"threshold\": threshold})\n",
    "```\n",
    "2. Track everything in MLflow:\n",
    "- Consensus threshold (e.g., 0.4)\n",
    "- Feature counts per fold\n",
    "- Importance scores averaged across folds\n",
    "- Final feature list\n",
    "3. Validate the threshold choice:\n",
    "- Plot: threshold (x-axis) vs. RMSE (y-axis)\n",
    "- Look for \"elbow\" where performance plateaus\n",
    "- Choose threshold that balances features vs. performance\n",
    "\n",
    "Bottom Line:\n",
    "\n",
    "For your PC price forecasting project with 150 monthly observations:\n",
    "\n",
    "Recommended Configuration:\n",
    "CV_CONFIG = {\n",
    "    'n_splits': 5,           # Standard\n",
    "    'gap': 3,                # 3-month embargo\n",
    "    'threshold': 0.4,        # 2/5 folds (lenient for small data)\n",
    "    'max_train_size': None   # Use all available history\n",
    "}\n",
    "\n",
    "Expected outcome:\n",
    "- ~10-15 features selected at 40% threshold\n",
    "- More robust than single-train selection\n",
    "- Computational cost: 5x (acceptable for your dataset size)\n",
    "- Interpretability: Can explain \"feature X was useful in 4/5 time periods\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Use a simple, fast model for feature selection, then use those features with your actual modeling approaches (XGBoost,\n",
    "Prophet, etc.).\n",
    "\n",
    "Two-Stage Process:\n",
    "\n",
    "# STAGE 1: Feature Selection (uses simple model)\n",
    "Goal: Reduce from 100+ features to ~15-20 robust features\n",
    "\n",
    "```python\n",
    "selected_features = select_features_with_cv(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    selector_model=RandomForestRegressor(  # <-- Simple model for selection\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "# STAGE 2: Actual Modeling (uses selected features)\n",
    "Goal: Find best model for prediction\n",
    "\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "# Try different models with the SAME feature set:\n",
    "```python\n",
    "models = {\n",
    "    'xgboost': XGBRegressor(...),\n",
    "    'random_forest': RandomForestRegressor(...),\n",
    "    'prophet': Prophet(...),\n",
    "    'sarima': SARIMAX(...)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    predictions = model.predict(X_test_selected)\n",
    "    # Compare performance...\n",
    "```\n",
    "\n",
    "Why This Works:\n",
    "\n",
    "1. Feature selection is model-agnostic (mostly)\n",
    "- Features that help Random Forest usually help XGBoost too\n",
    "- Both are tree-based and capture similar relationships\n",
    "- Good features for ML models often good for time-series models too\n",
    "2. Selection model should be:\n",
    "- Fast (you're running 5-fold CV)\n",
    "- Robust (not too prone to overfitting)\n",
    "- Similar to final models (if possible)\n",
    "\n",
    "Recommended Selection Models:\n",
    "\n",
    "| Your Final Models                | Use This For Selection                                | Why                                         |\n",
    "|----------------------------------|-------------------------------------------------------|---------------------------------------------|\n",
    "| XGBoost, CatBoost, Random Forest | RandomForestRegressor(n_estimators=100, max_depth=10) | Fast, similar algorithm family, robust      |\n",
    "| Linear models (Ridge, Lasso)     | LassoCV or correlation-based                          | Matches model assumptions                   |\n",
    "| Prophet, SARIMA                  | Correlation + domain knowledge                        | Time-series models use features differently |\n",
    "| Mixed (trying everything)        | RandomForestRegressor                                 | Most versatile, works for all               |\n",
    "\n",
    "For Your PC Price Project:\n",
    "\n",
    "Recommended approach:\n",
    "\n",
    "## Stage 1: Feature Selection\n",
    "Use Random Forest because you'll try multiple model types\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "def select_features_cv(X, y, n_splits=5, threshold=0.4):\n",
    "    \"\"\"\n",
    "    Select features using Random Forest in CV folds.\n",
    "    \"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits, gap=3)\n",
    "    feature_counts = Counter()\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "\n",
    "        # Simple, fast Random Forest for selection\n",
    "        rf = RandomForestRegressor(\n",
    "            n_estimators=100,      # Enough for stable importance\n",
    "            max_depth=10,          # Prevent overfitting\n",
    "            min_samples_leaf=5,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "        # Keep top 50% by importance\n",
    "        threshold_imp = np.percentile(rf.feature_importances_, 50)\n",
    "        selected = X_train.columns[rf.feature_importances_ > threshold_imp]\n",
    "\n",
    "        feature_counts.update(selected)\n",
    "\n",
    "    # Return features appearing in ≥40% of folds\n",
    "    min_appearances = int(threshold * n_splits)  # 2/5 folds\n",
    "    robust_features = [\n",
    "        feat for feat, count in feature_counts.items()\n",
    "        if count >= min_appearances\n",
    "    ]\n",
    "\n",
    "    return robust_features\n",
    "\n",
    "# Use it:\n",
    "selected_features = select_features_cv(X_train, y_train)\n",
    "\n",
    "print(f\"Selected {len(selected_features)} features from {len(X_train.columns)}\")\n",
    "```\n",
    "\n",
    "Then:\n",
    "\n",
    "## Stage 2: Model Comparison\n",
    "Try different models with the SAME selected features\n",
    "\n",
    "```python\n",
    "X_train_sel = X_train[selected_features]\n",
    "X_test_sel = X_test[selected_features]\n",
    "\n",
    "models_to_compare = {\n",
    "    'xgboost': XGBRegressor(n_estimators=1000, learning_rate=0.01),\n",
    "    'catboost': CatBoostRegressor(iterations=1000, learning_rate=0.05),\n",
    "    'random_forest': RandomForestRegressor(n_estimators=500),\n",
    "    # Prophet and SARIMA handle features differently, see below\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models_to_compare.items():\n",
    "    model.fit(X_train_sel, y_train)\n",
    "    preds = model.predict(X_test_sel)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "    results[name] = rmse\n",
    "\n",
    "    # Log to MLflow\n",
    "    with mlflow.start_run(run_name=f\"{name}_h3\"):\n",
    "        mlflow.log_param(\"model_type\", name)\n",
    "        mlflow.log_param(\"n_features\", len(selected_features))\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "```\n",
    "\n",
    "### Special Case: Time-Series Models (Prophet, SARIMA)\n",
    "\n",
    "Important: Prophet and SARIMA use exogenous features differently:\n",
    "\n",
    "#### For Prophet:\n",
    "Only use features that make sense as regressors\n",
    "\n",
    "```python\n",
    "prophet_features = [f for f in selected_features\n",
    "                    if 'lag' not in f]  # Prophet handles lags internally\n",
    "\n",
    "model = Prophet()\n",
    "for feat in prophet_features:\n",
    "    model.add_regressor(feat)\n",
    "\n",
    "# Prepare data in Prophet format\n",
    "prophet_df = pd.DataFrame({\n",
    "    'ds': train_dates,\n",
    "    'y': y_train,\n",
    "    **{feat: X_train_sel[feat] for feat in prophet_features}\n",
    "})\n",
    "model.fit(prophet_df)\n",
    "```\n",
    "\n",
    "#### For SARIMA:\n",
    "Use selected features as exogenous variables\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "model = SARIMAX(\n",
    "    y_train,\n",
    "    exog=X_train_sel[selected_features],  # Use all selected features\n",
    "    order=(1, 1, 1),\n",
    "    seasonal_order=(1, 1, 1, 12)\n",
    ")\n",
    "results = model.fit()\n",
    "```\n",
    "\n",
    "Alternative: Model-Specific Selection\n",
    "\n",
    "If you want to be more rigorous, you could select features separately for each model type:\n",
    "\n",
    "- Option 1: One feature set for all models (simpler)\n",
    "features_all = select_features_cv(X, y, selector_model=RandomForestRegressor())\n",
    "\n",
    "- Option 2: Model-specific feature sets (more complex)\n",
    "features_for_xgboost = select_features_cv(X, y, selector_model=XGBRegressor())\n",
    "features_for_linear = select_features_cv(X, y, selector_model=LassoCV())\n",
    "features_for_ts = select_features_correlation(X, y)  # Correlation-based\n",
    "\n",
    "Then train each model with its own features\n",
    "\n",
    "Recommendation: Start with Option 1 (single feature set using Random Forest). Only use Option 2 if you find significant performance\n",
    "differences.\n",
    "\n",
    "Summary:\n",
    "\n",
    "- Feature selection model = Simple, fast model to identify important features (Random Forest)\n",
    "- Final prediction models = The actual models you'll compare (XGBoost, CatBoost, Prophet, SARIMA, etc.)\n",
    "\n",
    "Think of it as:\n",
    "- Feature selection = \"Which ingredients are important?\" (use a simple recipe)\n",
    "- Final modeling = \"What's the best way to cook those ingredients?\" (try multiple recipes)\n",
    "\n",
    "The feature selection step just reduces dimensionality and removes noise. The final modeling step is where you actually optimize for\n",
    "prediction performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "14-data-challenge (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
