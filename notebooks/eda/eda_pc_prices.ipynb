{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# EDA: Polycarbonate Prices\n",
    "\n",
    "This notebook performs exploratory data analysis on polycarbonate (PC) prices from Asian and European suppliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "# Configure paths\n",
    "from config.paths import RAW_DATA_DIR  # noqa: E402"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "We use the [FEDR](https://fred.stlouisfed.org/series/DEXCHUS) exchange rate data to convert prices to a common currency (USD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PC price data\n",
    "pc_price_dir = RAW_DATA_DIR / \"pc_price\"\n",
    "\n",
    "df_pc_asia = pd.read_csv(pc_price_dir / \"pc_price_asia.csv\", sep=\";\")\n",
    "df_pc_eu = pd.read_csv(pc_price_dir / \"pc_price_eu.csv\")\n",
    "\n",
    "df_conversion_rates = pd.read_csv(RAW_DATA_DIR / \"DEXCHUS.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Cleaning the names of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pc_eu.columns = df_pc_eu.columns.str.strip()\n",
    "df_pc_asia.columns = df_pc_asia.columns.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Cleaning the date columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conversion_rates = df_conversion_rates.copy()\n",
    "df_conversion_rates[\"observation_date\"] = pd.to_datetime(\n",
    "    df_conversion_rates[\"observation_date\"], format=\"%Y-%m-%d\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pc_asia = df_pc_asia.copy()\n",
    "df_pc_asia[\"Date\"] = pd.to_datetime(df_pc_asia[\"Date\"], format=\"%Y-%m-%d\")\n",
    "df_pc_asia = df_pc_asia.drop(columns=[\"Year\", \"Month\", \"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pc_eu = df_pc_eu.copy()\n",
    "df_pc_eu[\"Date\"] = pd.to_datetime(df_pc_eu[\"Date\"], format=\"%Y-%m-%d\")\n",
    "df_pc_eu = df_pc_eu.drop(columns=[\"Year\", \"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview data\n",
    "print(\"Asia - First few rows:\")\n",
    "display(df_pc_asia.head())\n",
    "\n",
    "print(\"\\nEurope - First few rows:\")\n",
    "display(df_pc_eu.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Let's look at the range spread in both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize date ranges for both datasets\n",
    "fig = go.Figure()\n",
    "\n",
    "# Get date ranges\n",
    "asia_start = df_pc_asia[\"Date\"].min()\n",
    "asia_end = df_pc_asia[\"Date\"].max()\n",
    "eu_start = df_pc_eu[\"Date\"].min()\n",
    "eu_end = df_pc_eu[\"Date\"].max()\n",
    "\n",
    "# Add horizontal bars for date ranges\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[asia_start, asia_end],\n",
    "        y=[\"Asia Dataset\", \"Asia Dataset\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"Asia\",\n",
    "        line=dict(color=\"coral\", width=20),\n",
    "        hovertemplate=\"<b>Asia</b><br>%{x|%Y-%m-%d}<extra></extra>\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[eu_start, eu_end],\n",
    "        y=[\"Europe Dataset\", \"Europe Dataset\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"Europe\",\n",
    "        line=dict(color=\"steelblue\", width=20),\n",
    "        hovertemplate=\"<b>Europe</b><br>%{x|%Y-%m-%d}<extra></extra>\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"PC Price Dataset - Date Range Comparison\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Dataset\",\n",
    "    height=400,\n",
    "    template=\"plotly_white\",\n",
    "    showlegend=True,\n",
    "    hovermode=\"closest\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Let's take the exchange rates from FEDR to convert all prices to USD/Kg for the dates in the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_eu = df_pc_eu[\"Date\"]\n",
    "dates_asia = df_pc_asia[\"Date\"]\n",
    "all_pc_dates = pd.to_datetime(np.unique(np.concatenate([dates_eu, dates_asia])))\n",
    "\n",
    "df_conversion_rates = df_conversion_rates[\n",
    "    df_conversion_rates[\"observation_date\"].isin(all_pc_dates)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data size\n",
    "print(f\"Asia - Data shape: {df_pc_asia.shape}\")\n",
    "print(f\"Europe - Data shape: {df_pc_eu.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Missing Values Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Visually, there appears to be a lot of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null values analysis\n",
    "asia_missing_values_percentage = round(\n",
    "    df_pc_asia.isna().sum() / len(df_pc_asia) * 100, 2\n",
    ")\n",
    "\n",
    "eu_missing_values_percentage = round(df_pc_eu.isna().sum() / len(df_pc_eu) * 100, 2)\n",
    "\n",
    "print(\"Asia - Missing values percentage:\")\n",
    "display(asia_missing_values_percentage.sort_values(ascending=False))\n",
    "\n",
    "print(\"\\nEurope - Missing values percentage:\")\n",
    "display(eu_missing_values_percentage.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Let's drop columns with too many missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "asia_missing_mask = asia_missing_values_percentage == 100.0\n",
    "eu_missing_mask = eu_missing_values_percentage == 100.0\n",
    "\n",
    "# Drop columns with 100% missing values\n",
    "df_pc_asia = df_pc_asia.loc[:, ~asia_missing_mask]\n",
    "df_pc_eu = df_pc_eu.loc[:, ~eu_missing_mask]\n",
    "\n",
    "# For EU, we also drop the unamed column\n",
    "df_pc_eu = df_pc_eu.drop(columns=[\"Unnamed: 49\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Removing unecessary columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "As discussed with the client, we will drop columns ending with \"%\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop_asia = [col for col in df_pc_asia.columns if col.endswith(\"%\")]\n",
    "cols_to_drop_eu = [col for col in df_pc_eu.columns if col.endswith(\"%\")]\n",
    "\n",
    "df_pc_asia = df_pc_asia.drop(columns=cols_to_drop_asia)\n",
    "df_pc_eu = df_pc_eu.drop(columns=cols_to_drop_eu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Fill missing values in conversion rates for Asia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_map = df_conversion_rates.set_index(\"observation_date\")[\"DEXCHUS\"].to_dict()\n",
    "mask = df_pc_asia[\"USD/RMB\"].isna() | (df_pc_asia[\"USD/RMB\"] == 0)\n",
    "df_pc_asia.loc[mask, \"USD/RMB\"] = df_pc_asia.loc[mask, \"Date\"].map(conversion_map)\n",
    "\n",
    "# For remaining missing values, we use forward fill method\n",
    "df_pc_asia = df_pc_asia.sort_values(by=\"Date\").reset_index(drop=True)\n",
    "df_pc_asia[\"USD/RMB\"] = df_pc_asia[\"USD/RMB\"].fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Transformation: Grouping by PC type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "The PC prices datasets contain multiple types of polycarbonate, and multiple suppliers for each type. For each PC type, we will consider the minimum price across all suppliers as the representative price for that type (as discussed with the client)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_eu_crystal = [col for col in df_pc_eu.columns if \"crystal\" in col.lower()]\n",
    "cols_eu_white = [col for col in df_pc_eu.columns if \"white\" in col.lower()]\n",
    "cols_eu_gf10 = [\n",
    "    col\n",
    "    for col in df_pc_eu.columns\n",
    "    if (\"gf10\" in col.lower() or \"gf 10\" in col.lower())\n",
    "    and \"recyclÃ©\" not in col.lower()\n",
    "]\n",
    "cols_eu_gf20 = [col for col in df_pc_eu.columns if \"gf20\" in col.lower()]\n",
    "cols_eu_si = [col for col in df_pc_eu.columns if \"si\" in col.lower()]\n",
    "cols_eu_recycled_white = [\n",
    "    col\n",
    "    for col in df_pc_eu.columns\n",
    "    if \"recyclÃ©\" in col.lower() and \"white\" in col.lower()\n",
    "]\n",
    "cols_eu_recycled_grey = [\n",
    "    col\n",
    "    for col in df_pc_eu.columns\n",
    "    if \"recyclÃ©\" in col.lower() and \"grey\" in col.lower()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pc_eu[\"PC_Crystal_best\"] = df_pc_eu[cols_eu_crystal].min(axis=1)\n",
    "df_pc_eu[\"PC_White_best\"] = df_pc_eu[cols_eu_white].min(axis=1)\n",
    "df_pc_eu[\"PC_GF10_best\"] = df_pc_eu[cols_eu_gf10].min(axis=1)\n",
    "df_pc_eu[\"PC_GF20_best\"] = df_pc_eu[cols_eu_gf20].min(axis=1)\n",
    "df_pc_eu[\"PC_Si_best\"] = df_pc_eu[cols_eu_si].min(axis=1)\n",
    "df_pc_eu[\"PC_Recycled_white_best\"] = df_pc_eu[cols_eu_recycled_white].min(axis=1)\n",
    "df_pc_eu[\"PC_Recycled_grey_best\"] = df_pc_eu[cols_eu_recycled_grey].min(axis=1)\n",
    "\n",
    "df_pc_eu = df_pc_eu.drop(\n",
    "    columns=cols_eu_crystal\n",
    "    + cols_eu_white\n",
    "    + cols_eu_gf10\n",
    "    + cols_eu_gf20\n",
    "    + cols_eu_si\n",
    "    + cols_eu_recycled_white\n",
    "    + cols_eu_recycled_grey\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pc_eu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### Asia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "We want the same currency for all prices, so we will convert all prices to USD/Kg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all price columns (exclude Date, USD/RMB, and non-price columns)\n",
    "price_columns = [\n",
    "    col\n",
    "    for col in df_pc_asia.columns\n",
    "    if col not in [\"Date\", \"USD/RMB\", \"Year\", \"Month\", \"date\"]\n",
    "    and \"SPREAD\" not in col\n",
    "    and \"ICIS\" not in col\n",
    "    and \"CHEMSINO\" not in col\n",
    "    and \"TECNON\" not in col\n",
    "]\n",
    "\n",
    "\n",
    "def get_supplier_pc_info(col_name):\n",
    "    \"\"\"Extract supplier name and PC type from column name.\"\"\"\n",
    "    # Extract PC type (GP, FR, GF, etc.)\n",
    "    pc_type = None\n",
    "    for pt in [\n",
    "        \"GP recycled\",\n",
    "        \"GP\",\n",
    "        \"GF10FR\",\n",
    "        \"GF recycled\",\n",
    "        \"GF\",\n",
    "        \"FR\",\n",
    "        \"NAT\",\n",
    "        \"Si recycled\",\n",
    "        \"Si\",\n",
    "    ]:\n",
    "        if pt in col_name:\n",
    "            pc_type = pt\n",
    "            break\n",
    "\n",
    "    # Extract supplier name\n",
    "    supplier = None\n",
    "    for i in range(1, 10):\n",
    "        if f\"asia_supplier_{i}\" in col_name:\n",
    "            supplier = f\"asia_supplier_{i}\"\n",
    "            break\n",
    "\n",
    "    return supplier, pc_type\n",
    "\n",
    "\n",
    "converted_cols = {}\n",
    "conversion_stats = {\n",
    "    \"kept_usd_kg\": 0,\n",
    "    \"converted_rmb_t\": 0,\n",
    "    \"converted_rmb_kg\": 0,\n",
    "    \"already_usd\": 0,\n",
    "    \"errors\": [],\n",
    "}\n",
    "\n",
    "# Process each column\n",
    "for col in price_columns:\n",
    "    supplier, pc_type = get_supplier_pc_info(col)\n",
    "\n",
    "    # Determine unit from column name\n",
    "    if \"RMB/T\" in col or \"RMB/t\" in col:\n",
    "        unit = \"RMB/T\"\n",
    "    elif \"RMB/KG\" in col or \"RMB/kg\" in col or \"RMB/Kg\" in col:\n",
    "        unit = \"RMB/KG\"\n",
    "    elif \"USD/KG\" in col or \"USD/kg\" in col or \"USD/Kg\" in col:\n",
    "        unit = \"USD/KG\"\n",
    "    elif \"INR/KG\" in col or \"INR/kg\" in col or \"INR/Kg\" in col:\n",
    "        unit = \"INR/KG\"\n",
    "    else:\n",
    "        # Default assumption: USD/Kg if no unit specified\n",
    "        unit = \"USD/KG\"\n",
    "\n",
    "    # Handle columns without supplier name (like \"PC Si (RMB/kg)\")\n",
    "    if supplier is None or pc_type is None:\n",
    "        # Try to extract just the PC type for columns without supplier\n",
    "        if \" Si recycled \" in col or col.startswith(\"PC Si recycled\"):\n",
    "            pc_type_only = \"Si recycled\"\n",
    "        elif \" Si \" in col or col.startswith(\"PC Si\"):\n",
    "            pc_type_only = \"Si\"\n",
    "        else:\n",
    "            # Can't parse this column\n",
    "            if unit == \"INR/KG\":\n",
    "                print(f\"âš ï¸  Skipping {col} - INR conversion not implemented\")\n",
    "            else:\n",
    "                print(f\"âš ï¸  Skipping {col} - couldn't parse supplier/type\")\n",
    "            conversion_stats[\"errors\"].append(col)\n",
    "            continue\n",
    "\n",
    "        # Create column name without supplier\n",
    "        new_col_name = f\"PC {pc_type_only} (USD/Kg)\"\n",
    "    else:\n",
    "        # Create standardized column name with supplier\n",
    "        new_col_name = f\"{supplier} PC {pc_type} (USD/Kg)\"\n",
    "\n",
    "    # Skip INR conversions\n",
    "    if unit == \"INR/KG\":\n",
    "        print(f\"âš ï¸  Skipping {col} - INR conversion not implemented\")\n",
    "        conversion_stats[\"errors\"].append(col)\n",
    "        continue\n",
    "\n",
    "    # Convert based on unit\n",
    "    if unit == \"USD/KG\":\n",
    "        # Already in USD/Kg, just copy\n",
    "        converted_cols[new_col_name] = df_pc_asia[col].copy()\n",
    "        conversion_stats[\"already_usd\"] += 1\n",
    "    elif unit == \"RMB/T\":\n",
    "        # Convert from RMB/T to USD/Kg\n",
    "        # RMB/T -> USD/T (divide by USD/RMB) -> USD/Kg (divide by 1000)\n",
    "        converted_cols[new_col_name] = df_pc_asia[col] / df_pc_asia[\"USD/RMB\"] / 1000\n",
    "        conversion_stats[\"converted_rmb_t\"] += 1\n",
    "    elif unit == \"RMB/KG\":\n",
    "        # Convert from RMB/Kg to USD/Kg\n",
    "        converted_cols[new_col_name] = df_pc_asia[col] / df_pc_asia[\"USD/RMB\"]\n",
    "        conversion_stats[\"converted_rmb_kg\"] += 1\n",
    "\n",
    "# For duplicate columns (same supplier, same PC type), prefer USD/Kg values when\n",
    "# available\n",
    "final_cols = {}\n",
    "for col_name, values in converted_cols.items():\n",
    "    if col_name in final_cols:\n",
    "        # Merge: prefer non-null values from USD/Kg column\n",
    "        final_cols[col_name] = values.combine_first(final_cols[col_name])\n",
    "        conversion_stats[\"kept_usd_kg\"] += 1\n",
    "    else:\n",
    "        final_cols[col_name] = values\n",
    "\n",
    "# Create new dataframe starting with original columns\n",
    "df_pc_asia_converted = df_pc_asia.copy()\n",
    "\n",
    "# Track which original columns were converted\n",
    "converted_original_cols = []\n",
    "for col in price_columns:\n",
    "    supplier, pc_type = get_supplier_pc_info(col)\n",
    "\n",
    "    # Try both with and without supplier names\n",
    "    possible_new_names = []\n",
    "\n",
    "    if supplier is not None and pc_type is not None:\n",
    "        possible_new_names.append(f\"{supplier} PC {pc_type} (USD/Kg)\")\n",
    "\n",
    "    # Also check for columns without supplier (like \"PC Si (USD/Kg)\")\n",
    "    if \" Si recycled \" in col or col.startswith(\"PC Si recycled\"):\n",
    "        possible_new_names.append(\"PC Si recycled (USD/Kg)\")\n",
    "    elif \" Si \" in col or col.startswith(\"PC Si\"):\n",
    "        possible_new_names.append(\"PC Si (USD/Kg)\")\n",
    "\n",
    "    # Check if any of the possible names exists in final_cols\n",
    "    for new_col_name in possible_new_names:\n",
    "        if new_col_name in final_cols:\n",
    "            converted_original_cols.append(col)\n",
    "            break\n",
    "\n",
    "# Remove only the columns that were successfully converted\n",
    "df_pc_asia_converted = df_pc_asia_converted.drop(columns=converted_original_cols)\n",
    "\n",
    "# Add all converted price columns\n",
    "for col_name, values in sorted(final_cols.items()):\n",
    "    df_pc_asia_converted[col_name] = values\n",
    "\n",
    "# Replace original dataframe\n",
    "df_pc_asia = df_pc_asia_converted.copy()\n",
    "\n",
    "print(\"\\nâœ… Conversion complete!\")\n",
    "print(\"\\nConversion statistics:\")\n",
    "print(f\"  - Already in USD/Kg: {conversion_stats['already_usd']} columns\")\n",
    "print(f\"  - Converted from RMB/T: {conversion_stats['converted_rmb_t']} columns\")\n",
    "print(f\"  - Converted from RMB/Kg: {conversion_stats['converted_rmb_kg']} columns\")\n",
    "print(f\"  - Merged duplicate USD/Kg: {conversion_stats['kept_usd_kg']} columns\")\n",
    "if conversion_stats[\"errors\"]:\n",
    "    print(f\"  - Errors/Skipped: {len(conversion_stats['errors'])} columns (kept as-is)\")\n",
    "\n",
    "print(\"\\nðŸ“Š Final dataset:\")\n",
    "print(f\"  - Shape: {df_pc_asia.shape}\")\n",
    "print(f\"  - Converted columns: {len(final_cols)}\")\n",
    "print(f\"  - Original columns removed: {len(converted_original_cols)}\")\n",
    "print(\n",
    "    f\"  - Columns kept unchanged: {df_pc_asia.shape[1] - len(final_cols) - 2}\"\n",
    ")  # -2 for Date and USD/RMB\n",
    "print(f\"  - Date range: {df_pc_asia['Date'].min()} to {df_pc_asia['Date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converted price columns (USD/Kg)\n",
    "price_cols_asia = [\n",
    "    col\n",
    "    for col in df_pc_asia.columns\n",
    "    if col.endswith(\"(USD/Kg)\") and col not in [\"Date\", \"USD/RMB\"]\n",
    "]\n",
    "\n",
    "# GP\n",
    "cols_asia_gp = [\n",
    "    col for col in price_cols_asia if \" GP \" in col and \"recycled\" not in col.lower()\n",
    "]\n",
    "\n",
    "# GP Recycled\n",
    "cols_asia_gp_recycled = [\n",
    "    col for col in price_cols_asia if \" GP \" in col and \"recycled\" in col.lower()\n",
    "]\n",
    "\n",
    "# FR\n",
    "cols_asia_fr = [\n",
    "    col\n",
    "    for col in price_cols_asia\n",
    "    if \" FR \" in col and \"GF\" not in col  # Exclude GF10FR\n",
    "]\n",
    "\n",
    "# GF - includes GF10FR\n",
    "cols_asia_gf = [\n",
    "    col\n",
    "    for col in price_cols_asia\n",
    "    if (\" GF \" in col or \"GF10FR\" in col) and \"recycled\" not in col.lower()\n",
    "]\n",
    "\n",
    "# GF Recycled\n",
    "cols_asia_gf_recycled = [\n",
    "    col for col in price_cols_asia if \" GF \" in col and \"recycled\" in col.lower()\n",
    "]\n",
    "\n",
    "# NAT\n",
    "cols_asia_nat = [col for col in price_cols_asia if \" NAT \" in col]\n",
    "\n",
    "# Si\n",
    "cols_asia_si = [\n",
    "    col for col in price_cols_asia if \" Si \" in col and \"recycled\" not in col.lower()\n",
    "]\n",
    "\n",
    "# Si Recycled\n",
    "cols_asia_si_recycled = [\n",
    "    col for col in price_cols_asia if \" Si \" in col and \"recycled\" in col.lower()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pc_asia[\"PC_GP_best\"] = df_pc_asia[cols_asia_gp].min(axis=1)\n",
    "df_pc_asia[\"PC_GP_Recycled_best\"] = df_pc_asia[cols_asia_gp_recycled].min(axis=1)\n",
    "df_pc_asia[\"PC_FR_best\"] = df_pc_asia[cols_asia_fr].min(axis=1)\n",
    "df_pc_asia[\"PC_GF_best\"] = df_pc_asia[cols_asia_gf].min(axis=1)\n",
    "df_pc_asia[\"PC_GF_Recycled_best\"] = df_pc_asia[cols_asia_gf_recycled].min(axis=1)\n",
    "df_pc_asia[\"PC_NAT_best\"] = df_pc_asia[cols_asia_nat].min(axis=1)\n",
    "df_pc_asia[\"PC_Si_best\"] = df_pc_asia[cols_asia_si].min(axis=1)\n",
    "df_pc_asia[\"PC_Si_Recycled_best\"] = df_pc_asia[cols_asia_si_recycled].min(axis=1)\n",
    "\n",
    "df_pc_asia = df_pc_asia.drop(\n",
    "    columns=cols_asia_gp\n",
    "    + cols_asia_gp_recycled\n",
    "    + cols_asia_fr\n",
    "    + cols_asia_gf\n",
    "    + cols_asia_gf_recycled\n",
    "    + cols_asia_nat\n",
    "    + cols_asia_si\n",
    "    + cols_asia_si_recycled\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cleaned Asia PC Prices DataFrame:\")\n",
    "display(df_pc_asia.head())\n",
    "\n",
    "print(\"\\nCleaned Europe PC Prices DataFrame:\")\n",
    "display(df_pc_eu.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### Plots of best prices over time per PC type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "#### Europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "# Get all PC type columns (ending with \"_best\")\n",
    "pc_type_cols_eu = [col for col in df_pc_eu.columns if col.endswith(\"_best\")]\n",
    "\n",
    "# Define color palette\n",
    "colors = [\n",
    "    \"#1f77b4\",  # blue\n",
    "    \"#ff7f0e\",  # orange\n",
    "    \"#2ca02c\",  # green\n",
    "    \"#d62728\",  # red\n",
    "    \"#9467bd\",  # purple\n",
    "    \"#8c564b\",  # brown\n",
    "    \"#e377c2\",  # pink\n",
    "]\n",
    "\n",
    "# Add trace for each PC type\n",
    "for i, col in enumerate(pc_type_cols_eu):\n",
    "    # Clean up column name for legend\n",
    "    pc_type_name = col.replace(\"PC_\", \"\").replace(\"_best\", \"\").replace(\"_\", \" \")\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_pc_eu[\"Date\"],\n",
    "            y=df_pc_eu[col],\n",
    "            mode=\"lines+markers\",\n",
    "            name=pc_type_name,\n",
    "            line=dict(width=2, color=colors[i % len(colors)]),\n",
    "            marker=dict(size=4),\n",
    "            hovertemplate=\"<b>%{fullData.name}</b><br>\"\n",
    "            + \"Date: %{x|%Y-%m-%d}<br>\"\n",
    "            + \"Price: ?%{y:.2f}/kg<br>\"\n",
    "            + \"<extra></extra>\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"European PC Prices Evolution by Type\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Price (?/kg)\",\n",
    "    hovermode=\"x unified\",\n",
    "    height=600,\n",
    "    template=\"plotly_white\",\n",
    "    legend=dict(\n",
    "        title=\"PC Type\",\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=0.01,\n",
    "        bgcolor=\"rgba(255, 255, 255, 0.8)\",\n",
    "        bordercolor=\"lightgray\",\n",
    "        borderwidth=1,\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "#### Asia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "# Get all PC type columns (ending with \"_best\")\n",
    "pc_type_cols_asia = [col for col in df_pc_asia.columns if col.endswith(\"_best\")]\n",
    "\n",
    "# Define color palette\n",
    "colors = [\n",
    "    \"#1f77b4\",  # blue\n",
    "    \"#ff7f0e\",  # orange\n",
    "    \"#2ca02c\",  # green\n",
    "    \"#d62728\",  # red\n",
    "    \"#9467bd\",  # purple\n",
    "    \"#8c564b\",  # brown\n",
    "    \"#e377c2\",  # pink\n",
    "    \"#bcbd22\",  # olive\n",
    "]\n",
    "\n",
    "# Add trace for each PC type\n",
    "for i, col in enumerate(pc_type_cols_asia):\n",
    "    # Clean up column name for legend\n",
    "    pc_type_name = col.replace(\"PC_\", \"\").replace(\"_best\", \"\").replace(\"_\", \" \")\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_pc_asia[\"Date\"],\n",
    "            y=df_pc_asia[col],\n",
    "            mode=\"lines+markers\",\n",
    "            name=pc_type_name,\n",
    "            line=dict(width=2, color=colors[i % len(colors)]),\n",
    "            marker=dict(size=4),\n",
    "            hovertemplate=\"<b>%{fullData.name}</b><br>\"\n",
    "            + \"Date: %{x|%Y-%m-%d}<br>\"\n",
    "            + \"Price: â‚¬%{y:.2f}/kg<br>\"\n",
    "            + \"<extra></extra>\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Asian PC Prices Evolution by Type\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Price (USD/kg)\",\n",
    "    hovermode=\"x unified\",\n",
    "    height=600,\n",
    "    template=\"plotly_white\",\n",
    "    legend=dict(\n",
    "        title=\"PC Type\",\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=0.01,\n",
    "        bgcolor=\"rgba(255, 255, 255, 0.8)\",\n",
    "        bordercolor=\"lightgray\",\n",
    "        borderwidth=1,\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "- GF Recycled prices are false\n",
    "- GP prices always decreasing?\n",
    "- Si prices higher than other types --> Coherent with EU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "### Correlation Analysis Between columns of the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "#### Europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis - Europe Dataset\n",
    "# Get numeric columns (exclude Date)\n",
    "numeric_cols_eu = [\n",
    "    col\n",
    "    for col in df_pc_eu.columns\n",
    "    if col != \"Date\" and df_pc_eu[col].dtype in [\"float64\", \"int64\"]\n",
    "]\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix_eu = df_pc_eu[numeric_cols_eu].corr()\n",
    "\n",
    "# Create figure\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(\n",
    "    corr_matrix_eu,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"RdBu_r\",\n",
    "    center=0,\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\": 0.8, \"label\": \"Correlation\"},\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    ")\n",
    "\n",
    "plt.title(\n",
    "    \"Correlation Matrix - European PC Prices\", fontsize=16, fontweight=\"bold\", pad=20\n",
    ")\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "#### Asia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis - Asia Dataset\n",
    "# Get numeric columns (exclude Date)\n",
    "numeric_cols_asia = [\n",
    "    col\n",
    "    for col in df_pc_asia.columns\n",
    "    if col != \"Date\" and df_pc_asia[col].dtype in [\"float64\", \"int64\"]\n",
    "]\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix_asia = df_pc_asia[numeric_cols_asia].corr()\n",
    "\n",
    "# Create figure\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(\n",
    "    corr_matrix_asia,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"RdBu_r\",\n",
    "    center=0,\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\": 0.8, \"label\": \"Correlation\"},\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    ")\n",
    "\n",
    "plt.title(\n",
    "    \"Correlation Matrix - European PC Prices\", fontsize=16, fontweight=\"bold\", pad=20\n",
    ")\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "### Self correlation analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "#### Europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocorrelation Analysis - Europe Dataset\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Get PC type columns\n",
    "pc_type_cols_eu = [col for col in df_pc_eu.columns if col.endswith(\"_best\")]\n",
    "\n",
    "# Create subplots for ACF and PACF for each PC type\n",
    "n_types = len(pc_type_cols_eu)\n",
    "fig, axes = plt.subplots(n_types, 2, figsize=(14, 4 * n_types))\n",
    "\n",
    "if n_types == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for i, col in enumerate(pc_type_cols_eu):\n",
    "    # Get non-null values\n",
    "    series = df_pc_eu[col].dropna()\n",
    "\n",
    "    # Clean up column name for title\n",
    "    pc_type_name = col.replace(\"PC_\", \"\").replace(\"_best\", \"\").replace(\"_\", \" \")\n",
    "\n",
    "    # Determine maximum lags based on series length\n",
    "    # ACF can use more lags, PACF limited to 50% of sample size\n",
    "    max_acf_lags = min(40, len(series) - 1)\n",
    "    max_pacf_lags = min(40, len(series) // 2 - 1)\n",
    "\n",
    "    # Plot ACF\n",
    "    plot_acf(series, lags=max_acf_lags, ax=axes[i, 0], alpha=0.05)\n",
    "    axes[i, 0].set_title(f\"ACF - {pc_type_name}\", fontweight=\"bold\")\n",
    "    axes[i, 0].set_xlabel(\"Lag\")\n",
    "    axes[i, 0].set_ylabel(\"Autocorrelation\")\n",
    "\n",
    "    # Plot PACF\n",
    "    plot_pacf(series, lags=max_pacf_lags, ax=axes[i, 1], alpha=0.05)\n",
    "    axes[i, 1].set_title(f\"PACF - {pc_type_name}\", fontweight=\"bold\")\n",
    "    axes[i, 1].set_xlabel(\"Lag\")\n",
    "    axes[i, 1].set_ylabel(\"Partial Autocorrelation\")\n",
    "\n",
    "    # Add data availability info\n",
    "    axes[i, 0].text(\n",
    "        0.02,\n",
    "        0.98,\n",
    "        f\"n={len(series)} ({len(series) / len(df_pc_eu) * 100:.1f}% available)\",\n",
    "        transform=axes[i, 0].transAxes,\n",
    "        verticalalignment=\"top\",\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.5),\n",
    "        fontsize=8,\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "  - **Crystal**: Differenciate once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "# Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "- Convert transformations into a pipeline \n",
    "- Complete data with info from other datasets --> Feature engineering and selection\n",
    "- Modeling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "14-data-challenge (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
