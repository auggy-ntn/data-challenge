{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Modeling SARIMA par PCType / région (multi_3m)\n",
    "Prévision mensuelle de `pc_price` par PCType/région avec SARIMA, split hold-out pour MAPE et option MLflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Imports et configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "import constants.constants as cst\n",
    "from constants.paths import PROCESSED_DATA_DIR\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 4)\n",
    "\n",
    "DATA_PATH = PROCESSED_DATA_DIR / \"multi_3m.csv\"\n",
    "assert DATA_PATH.exists(), DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Configurer MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "tracking_uri = os.getenv(\"MLFLOW_TRACKING_URI\")\n",
    "if tracking_uri:\n",
    "    mlflow.set_tracking_uri(tracking_uri)\n",
    "    print(f\"Tracking URI: {tracking_uri}\")\n",
    "\n",
    "experiment_id = os.getenv(\"MLFLOW_EXPERIMENT_ID\")\n",
    "experiment_name = os.getenv(\"MLFLOW_EXPERIMENT_NAME\")\n",
    "active_experiment_id = experiment_id\n",
    "\n",
    "if experiment_name:\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    exp = mlflow.get_experiment_by_name(experiment_name)\n",
    "    if exp and not active_experiment_id:\n",
    "        active_experiment_id = exp.experiment_id\n",
    "    print(\"Experiment name:\", experiment_name)\n",
    "\n",
    "if active_experiment_id:\n",
    "    print(\"Using experiment_id:\", active_experiment_id)\n",
    "else:\n",
    "    print(\"No experiment_id provided; runs will use the active/default experiment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Charger les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(horizon: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"Load multi_hm.csv and sort.\"\"\"\n",
    "    df_local = pd.read_csv(\n",
    "        PROCESSED_DATA_DIR / f\"multi_{horizon}m.csv\", parse_dates=[\"date\"]\n",
    "    )\n",
    "    return df_local.sort_values([\"pc_type\", \"region\", \"date\"])\n",
    "\n",
    "\n",
    "df = load_data(3)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Disponibilité des PCType (Enum vs données)\n",
    "\n",
    "Vérifie quelles valeurs de `PCType` existent dans les données processees et le coverage par région."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants.constants import PCType\n",
    "\n",
    "pc_enum = [p.value for p in PCType]\n",
    "pc_seen = sorted(df.pc_type.unique())\n",
    "counts = (\n",
    "    df.groupby([\"pc_type\", \"region\"])\n",
    "    .agg(start=(\"date\", \"min\"), end=(\"date\", \"max\"), rows=(\"date\", \"size\"))\n",
    "    .reset_index()\n",
    ")\n",
    "missing_enum = [p for p in pc_enum if p not in pc_seen]\n",
    "print(\"PCType vus dans les données:\", pc_seen)\n",
    "print(\"PCType Enum manquants dans les données:\", missing_enum)\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Fonction MAPE\n",
    "Utilisée pour les baselines et SARIMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true: pd.Series, y_pred: pd.Series) -> float | None:\n",
    "    \"\"\"MAPE en ignorant les zéros/NaN.\"\"\"\n",
    "    mask = (y_true != 0) & y_true.notna() & y_pred.notna()\n",
    "    if mask.sum() == 0:\n",
    "        return None\n",
    "    return (np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])).mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Baselines MAPE par horizon (naïf et saisonnier t-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_mape(\n",
    "    horizon: int,\n",
    "    strategy: str = \"naive\",\n",
    "    df_full: pd.DataFrame | None = None,\n",
    "    test_df: pd.DataFrame | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute naïf/saisonnier MAPE par pc_type/region.\"\"\"\n",
    "    rows = []\n",
    "    data = df_full if df_full is not None else df\n",
    "    for (pc, reg), g in data.groupby([\"pc_type\", \"region\"]):\n",
    "        g = g.sort_values(\"date\").set_index(\"date\")\n",
    "        y = g[\"pc_price\"]\n",
    "        if strategy == \"naive\":\n",
    "            pred = y.shift(horizon)\n",
    "        elif strategy == \"seasonal_naive\":\n",
    "            pred = y.shift(12)\n",
    "        else:\n",
    "            raise ValueError(\"strategy must be naive or seasonal_naive\")\n",
    "        if test_df is not None:\n",
    "            mask_dates = test_df[(test_df.pc_type == pc) & (test_df.region == reg)][\n",
    "                \"date\"\n",
    "            ]\n",
    "            test_mask = g.index.isin(mask_dates)\n",
    "            m = mape(y[test_mask], pred[test_mask])\n",
    "        else:\n",
    "            m = mape(y, pred)\n",
    "        rows.append(\n",
    "            {\n",
    "                \"pc_type\": pc,\n",
    "                \"region\": reg,\n",
    "                \"horizon\": horizon,\n",
    "                \"strategy\": strategy,\n",
    "                \"MAPE\": m,\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Split temporel adaptatif\n",
    "Assure un minimum d'observations train/test par (pc_type, region)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_train_test_split(\n",
    "    df_local: pd.DataFrame,\n",
    "    group_cols: list[str],\n",
    "    target_test_ratio: float = 0.2,\n",
    "    min_train_samples: int = 20,\n",
    "    min_test_samples: int = 5,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Split temporel tout en garantissant un minimum par groupe.\"\"\"\n",
    "    df_local = df_local.sort_values([\"date\"] + group_cols).copy()\n",
    "    split_dates = []\n",
    "    for _, g in df_local.groupby(group_cols):\n",
    "        g = g.sort_values(\"date\")\n",
    "        n_obs = len(g)\n",
    "        if n_obs < min_train_samples + min_test_samples:\n",
    "            split_dates.append(g[\"date\"].iloc[min_train_samples - 1])\n",
    "            continue\n",
    "        idx_train = max(min_train_samples, int(n_obs * (1 - target_test_ratio)))\n",
    "        idx_train = min(idx_train, n_obs - min_test_samples)\n",
    "        split_dates.append(g[\"date\"].iloc[idx_train - 1])\n",
    "    global_split = max(split_dates)\n",
    "    train_df = df_local[df_local[\"date\"] <= global_split]\n",
    "    test_df = df_local[df_local[\"date\"] > global_split]\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Baselines sur le jeu de validation (test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {}\n",
    "baseline_tables = []\n",
    "for horizon in [3, 6, 9]:\n",
    "    df_h = load_data(horizon)\n",
    "    train_df_h, test_df_h = adaptive_train_test_split(\n",
    "        df_h,\n",
    "        group_cols=[\"pc_type\", \"region\"],\n",
    "        target_test_ratio=0.2,\n",
    "        min_train_samples=20,\n",
    "        min_test_samples=5,\n",
    "    )\n",
    "    splits[horizon] = {\"df\": df_h, \"train\": train_df_h, \"test\": test_df_h}\n",
    "    baseline_tables.append(\n",
    "        baseline_mape(horizon, \"naive\", df_full=df_h, test_df=test_df_h)\n",
    "    )\n",
    "    baseline_tables.append(\n",
    "        baseline_mape(horizon, \"seasonal_naive\", df_full=df_h, test_df=test_df_h)\n",
    "    )\n",
    "baseline_df = pd.concat(baseline_tables, ignore_index=True)\n",
    "baseline_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Fonctions utilitaires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## SARIMA + logging MLflow (optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sarima(\n",
    "    pc_type: str,\n",
    "    region: str,\n",
    "    df_source: pd.DataFrame,\n",
    "    order: tuple[int, int, int] = (1, 1, 0),\n",
    "    seasonal_order: tuple[int, int, int, int] = (0, 0, 0, 12),\n",
    "    horizon: int = 6,\n",
    "    log_mlflow: bool = False,\n",
    "    experiment_id: str | None = None,\n",
    "    train_df: pd.DataFrame | None = None,\n",
    "    test_df: pd.DataFrame | None = None,\n",
    ") -> dict:\n",
    "    \"\"\"Fit SARIMA pour un pc_type/region et évalue sur y_test.\"\"\"\n",
    "    train_slice = (\n",
    "        train_df[(train_df.pc_type == pc_type) & (train_df.region == region)]\n",
    "        if train_df is not None\n",
    "        else df_source[(df_source.pc_type == pc_type) & (df_source.region == region)]\n",
    "    )\n",
    "    test_slice = (\n",
    "        test_df[(test_df.pc_type == pc_type) & (test_df.region == region)]\n",
    "        if test_df is not None\n",
    "        else None\n",
    "    )\n",
    "    train_slice = train_slice.sort_values(\"date\")\n",
    "    y_train = train_slice.set_index(\"date\")[\"pc_price\"]\n",
    "    y_test = None\n",
    "    if test_slice is not None and not test_slice.empty:\n",
    "        y_test = test_slice.sort_values(\"date\").set_index(\"date\")[\"pc_price\"]\n",
    "    if y_test is None:\n",
    "        if len(y_train) <= horizon + 2:\n",
    "            return {\n",
    "                \"pc_type\": pc_type,\n",
    "                \"region\": region,\n",
    "                \"MAPE\": None,\n",
    "                \"note\": \"série trop courte\",\n",
    "            }\n",
    "        y_test = y_train.iloc[-horizon:]\n",
    "        y_train = y_train.iloc[:-horizon]\n",
    "    if len(y_train) <= max(order[0], seasonal_order[0] * seasonal_order[3]):\n",
    "        return {\n",
    "            \"pc_type\": pc_type,\n",
    "            \"region\": region,\n",
    "            \"MAPE\": None,\n",
    "            \"note\": \"train trop court\",\n",
    "        }\n",
    "    try:\n",
    "        model = SARIMAX(\n",
    "            y_train,\n",
    "            order=order,\n",
    "            seasonal_order=seasonal_order,\n",
    "            enforce_stationarity=False,\n",
    "            enforce_invertibility=False,\n",
    "        )\n",
    "        res = model.fit(disp=False)\n",
    "        steps = min(len(y_test), horizon)\n",
    "        fc = res.get_forecast(steps=steps).predicted_mean\n",
    "        m = mape(y_test.iloc[:steps], fc)\n",
    "        result = {\n",
    "            \"pc_type\": pc_type,\n",
    "            \"region\": region,\n",
    "            \"MAPE\": m,\n",
    "            \"order\": order,\n",
    "            \"seasonal_order\": seasonal_order,\n",
    "            \"horizon\": horizon,\n",
    "        }\n",
    "        if log_mlflow:\n",
    "            with mlflow.start_run(\n",
    "                run_name=f\"SARIMA_{pc_type}_{region}_h{horizon}\",\n",
    "                experiment_id=experiment_id,\n",
    "            ):\n",
    "                mlflow.set_tags(\n",
    "                    {\n",
    "                        cst.MLFLOW_MODEL_TYPE: \"sarima\",\n",
    "                        cst.MLFLOW_MODEL_PHILOSOPHY: \"statistical\",\n",
    "                        cst.MLFLOW_HORIZON: horizon,\n",
    "                        cst.MLFLOW_FUNCTION: \"forecast\",\n",
    "                        \"pc_type\": pc_type,\n",
    "                        \"region\": region,\n",
    "                    }\n",
    "                )\n",
    "                mlflow.log_params(\n",
    "                    {\n",
    "                        \"order\": order,\n",
    "                        \"seasonal_order\": seasonal_order,\n",
    "                        \"horizon\": horizon,\n",
    "                    }\n",
    "                )\n",
    "                if m is not None:\n",
    "                    mlflow.log_metric(\"MAPE\", m)\n",
    "        return result\n",
    "    except Exception as exc:\n",
    "        return {\n",
    "            \"pc_type\": pc_type,\n",
    "            \"region\": region,\n",
    "            \"MAPE\": None,\n",
    "            \"error\": str(exc),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Run diagnostic (sans logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for horizon, objs in splits.items():\n",
    "    df_h = objs[\"df\"]\n",
    "    train_h = objs[\"train\"]\n",
    "    test_h = objs[\"test\"]\n",
    "    for pc in sorted(df_h.pc_type.unique()):\n",
    "        for reg in sorted(df_h.region.unique()):\n",
    "            results.append(\n",
    "                evaluate_sarima(\n",
    "                    pc, reg, df_h, horizon=horizon, train_df=train_h, test_df=test_h\n",
    "                )\n",
    "            )\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Run avec logging MLflow (optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_mlflow = []\n",
    "# for horizon, objs in splits.items():\n",
    "#     df_h = objs['df']\n",
    "#     train_h = objs['train']\n",
    "#     test_h = objs['test']\n",
    "#     for pc in sorted(df_h.pc_type.unique()):\n",
    "#         for reg in sorted(df_h.region.unique()):\n",
    "#             results_mlflow.append(\n",
    "#                 evaluate_sarima(\n",
    "#                     pc,\n",
    "#                     reg,\n",
    "#                     df_h,\n",
    "#                     horizon=horizon,\n",
    "#                     train_df=train_h,\n",
    "#                     test_df=test_h,\n",
    "#                     log_mlflow=True,\n",
    "#                     experiment_id=active_experiment_id,\n",
    "#                 )\n",
    "#             )\n",
    "# pd.DataFrame(results_mlflow)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "14-data-challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
