{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "The goal of this notebook is to setup a global model training framework, where a single model is trained on all pc types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Literal\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from constants import processed_names\n",
    "from constants.paths import PROCESSED_DATA_DIR\n",
    "from src.utils.data_split import adaptive_train_test_split\n",
    "from src.utils.logger import logger\n",
    "\n",
    "load_dotenv()\n",
    "mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# 1. Load and Prepare Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "We define a function to load data and separate features and target variable from the dataframe. There are different types of features:\n",
    "- Target variable: `pc_price`.\n",
    "- Meta features: `region`, `pc_type` and `date`. (Used for grouping and weighting but not as model features.)\n",
    "- Numerical features: `pc_price_lag_*`, `pc_price_rolling_mean_*`, `regional_avg_price`, `regional_price_volaility_`, `price_deviation_from_regional_avg`, exogenous features like `bpa_capacity_loss_kt` and their lags (less lags than for target), and time features like `month_sin`, `month_cos` and raw `month` or `year`.\n",
    "- Categorical binary features (can keep as is, tree based models handle $0$ and $1$):  `is_recycled`, `is_glass_filled`, `is_flame_retardant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(horizon: int = 3) -> tuple[pd.DataFrame, str, list[str]]:\n",
    "    \"\"\"Load processed data and separate features and target variable.\n",
    "\n",
    "    Args:\n",
    "        horizon (int, optional): Forecast horizon in months. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, str, list[str]]: DataFrame, target column name,\n",
    "        feature column names.\n",
    "    \"\"\"\n",
    "    # Load processed data\n",
    "    df = pd.read_csv(PROCESSED_DATA_DIR / f\"multi_{horizon}m.csv\")\n",
    "\n",
    "    # Separate features and target\n",
    "    target_col = processed_names.LONG_PC_PRICE\n",
    "    meta_cols = [\n",
    "        processed_names.LONG_DATE,\n",
    "        processed_names.LONG_REGION,\n",
    "        processed_names.LONG_PC_TYPE,\n",
    "    ]\n",
    "    feature_cols = [col for col in df.columns if col not in meta_cols + [target_col]]\n",
    "    logger.info(f\"Data loaded with {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "    logger.info(f\"Target: {target_col}. Features: {len(feature_cols)} columns.\")\n",
    "\n",
    "    return df, target_col, feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, target_col, feature_cols = load_and_prepare_data(horizon=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# 2. Split Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Because the data is imbalanced across different pc types, we need to ensure that the train set and the test set contain enough samples from each pc type. This is a problem especially for rare pc types (`gf20` notably). To do this, we use a function that performs an adaptive train-test split, ensuring that each pc type is represented in both sets with a minimum number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = adaptive_train_test_split(\n",
    "    df=df,\n",
    "    group_col=processed_names.LONG_PC_TYPE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# 3. Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(\n",
    "    train_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    feature_cols: list[str],\n",
    "    target_col: str,\n",
    "    horizon: int,\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Prepare features and target variable for training.\n",
    "\n",
    "    To align the target variable with the features, the target is shifted\n",
    "    by -horizon months within each group defined by region and pc_type.\n",
    "\n",
    "    Args:\n",
    "        train_df (pd.DataFrame): Training dataframe.\n",
    "        test_df (pd.DataFrame): Testing dataframe.\n",
    "        feature_cols (list[str]): List of feature column names.\n",
    "        target_col (str): Target column name.\n",
    "        horizon (int): Forecast horizon in months.\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray,\n",
    "        pd.DataFrame, pd.DataFrame]:\n",
    "            X_train, y_train, X_test, y_test, aligned training and testing dataframes\n",
    "    \"\"\"\n",
    "    # Target (shift by -horizon to align with features)\n",
    "    # Group by region and pc_type to shift correctly\n",
    "    train_df[\"target\"] = train_df.groupby(\n",
    "        [processed_names.LONG_REGION, processed_names.LONG_PC_TYPE]\n",
    "    )[target_col].shift(-horizon)\n",
    "    test_df[\"target\"] = test_df.groupby(\n",
    "        [processed_names.LONG_REGION, processed_names.LONG_PC_TYPE]\n",
    "    )[target_col].shift(-horizon)\n",
    "\n",
    "    # Drop rows with NaN in target (due to shifting)\n",
    "    train_mask = ~train_df[\"target\"].isna()\n",
    "    test_mask = ~test_df[\"target\"].isna()\n",
    "\n",
    "    X_train = train_df.loc[train_mask, feature_cols].values\n",
    "    y_train = train_df.loc[train_mask, \"target\"].values\n",
    "    train_df_aligned = train_df[train_mask].copy()\n",
    "    logger.info(f\"Training data prepared with {X_train.shape[0]} samples.\")\n",
    "\n",
    "    X_test = test_df.loc[test_mask, feature_cols].values\n",
    "    y_test = test_df.loc[test_mask, \"target\"].values\n",
    "    test_df_aligned = test_df[test_mask].copy()\n",
    "    logger.info(f\"Testing data prepared with {X_test.shape[0]} samples.\")\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, train_df_aligned, test_df_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, train_df_aligned, test_df_aligned = (\n",
    "    prepare_training_data(\n",
    "        train_df=train,\n",
    "        test_df=test,\n",
    "        feature_cols=feature_cols,\n",
    "        target_col=target_col,\n",
    "        horizon=3,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# 4. Compute Sample Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Because the data is imbalanced across different pc types, we compute sample weights to give more importance to under-represented pc types during model training. This helps the model to learn better representations for these rare pc types. Without this, the model might be biased towards the more common pc types, leading to poor performance on the rare ones. The global performance metric might be good, but the performance on rare pc types would be bad.\n",
    "\n",
    "Additionally, we can also weight samples based on region: we are only concerned about performance in Europe, so we can give more weight to samples from this region. We keep pc types from all regions in the training set to have more data, but we want to prioritize performance on European pc types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sample_weights(\n",
    "    df: pd.DataFrame,\n",
    "    group_col: str,\n",
    "    region_col: str,\n",
    "    target_region: str,\n",
    "    method: Literal[\"inverse_frequency\", \"sqrt_inverse\", \"balanced\"] = \"balanced\",\n",
    ") -> pd.Series:\n",
    "    \"\"\"Compute sample weights based on group frequency and region.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataframe containing the data.\n",
    "        group_col (str): Column name for grouping (e.g., pc_type).\n",
    "        region_col (str): Column name for region.\n",
    "        target_region (str): Region to prioritize.\n",
    "        method (Literal[\"inverse_frequency\", \"sqrt_inverse\", \"balanced\"], optional):\n",
    "            Method to compute weights. Defaults to \"balanced\".\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Sample weights for each row in the dataframe.\n",
    "    \"\"\"\n",
    "    # Validate method\n",
    "    if method not in [\"inverse_frequency\", \"sqrt_inverse\", \"balanced\"]:\n",
    "        raise ValueError(\n",
    "            f\"Invalid method: {method}. Choose from 'inverse_frequency', \"\n",
    "            \"'sqrt_inverse', 'balanced'.\"\n",
    "        )\n",
    "\n",
    "    group_counts = df[group_col].value_counts()\n",
    "    n_samples = len(df)\n",
    "    n_groups = len(group_counts)\n",
    "\n",
    "    # Simple inverse frequency weights (1/count)\n",
    "    if method == \"inverse_frequency\":\n",
    "        weights_map = {group: 1.0 / count for group, count in group_counts.items()}\n",
    "        weights = df[group_col].map(weights_map).values\n",
    "\n",
    "    # Square root of inverse frequency weights\n",
    "    # Less aggressive than inverse_frequency\n",
    "    elif method == \"sqrt_inverse\":\n",
    "        weights_map = {\n",
    "            group: 1.0 / np.sqrt(count) for group, count in group_counts.items()\n",
    "        }\n",
    "        weights = df[group_col].map(weights_map).values\n",
    "\n",
    "    elif method == \"balanced\":\n",
    "        # Sklearn-style balanced weights: n_samples / (n_groups * count)\n",
    "        # Normalized so sum(weights) â‰ˆ n_samples (maintains effective sample size)\n",
    "        weights_map = {\n",
    "            group: n_samples / (n_groups * count)\n",
    "            for group, count in group_counts.items()\n",
    "        }\n",
    "        weights = df[group_col].map(weights_map).values\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# 5. Define evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "# 6. Train Global Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "We train a single global model on all pc types using the computed sample weights, and log it to MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"test_model\"):\n",
    "    # Tags\n",
    "    mlflow.set_tags(\n",
    "        {\n",
    "            \"model.type\": \"global\",\n",
    "            \"model.algorithm\": \"xgboost\",\n",
    "            \"data.horizon\": 3,\n",
    "        }\n",
    "    )\n",
    "    # Compute sample weights\n",
    "    sample_weights = compute_sample_weights(\n",
    "        df=train_df_aligned,\n",
    "        group_col=processed_names.LONG_PC_TYPE,\n",
    "        region_col=processed_names.LONG_REGION,\n",
    "        target_region=\"Europe\",\n",
    "        method=\"balanced\",\n",
    "    )\n",
    "\n",
    "    # Train global model\n",
    "    from xgboost import XGBRegressor\n",
    "\n",
    "    global_model = XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        random_state=42,\n",
    "    )\n",
    "    global_model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        sample_weight=sample_weights,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        # verbose=True,\n",
    "    )\n",
    "\n",
    "    # Log model to MLflow\n",
    "    mlflow.xgboost.log_model(\n",
    "        global_model, name=\"test_global_model\", input_example=X_train[:5]\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "14-data-challenge (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
