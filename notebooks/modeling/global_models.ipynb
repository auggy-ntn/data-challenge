{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "The goal of this notebook is to setup a global model training framework, where a single model is trained on all pc types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Literal\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from constants import processed_names\n",
    "import constants.constants as cst\n",
    "from constants.paths import PROCESSED_DATA_DIR\n",
    "from src.utils.logger import logger\n",
    "from src.utils.multivariate_model_training import (\n",
    "    evaluate_and_log_model,\n",
    "    optimize_xgboost_model,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# 1. Load and Prepare Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "We define a function to load data and separate features and target variable from the dataframe. There are different types of features:\n",
    "- Target variable: `pc_price`.\n",
    "- Meta features: `region`, `pc_type` and `date`. (Used for grouping and weighting but not as model features.)\n",
    "- Numerical features: `pc_price_lag_*`, `pc_price_rolling_mean_*`, `regional_avg_price`, `regional_price_volaility_`, `price_deviation_from_regional_avg`, exogenous features like `bpa_capacity_loss_kt` and their lags (less lags than for target), and time features like `month_sin`, `month_cos` and raw `month` or `year`.\n",
    "- Categorical binary features (can keep as is, tree based models handle $0$ and $1$):  `is_recycled`, `is_glass_filled`, `is_flame_retardant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(\n",
    "    group_by_pc_types: bool, horizon: int = 3\n",
    ") -> tuple[pd.DataFrame, str, list[str]]:\n",
    "    \"\"\"Load processed data and separate features and target variable.\n",
    "\n",
    "    Args:\n",
    "        group_by_pc_types (bool): Whether PC prices are grouped by type.\n",
    "        horizon (int, optional): Forecast horizon in months. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, str, list[str]]: DataFrame, target column name,\n",
    "        feature column names.\n",
    "    \"\"\"\n",
    "    # Load processed data\n",
    "    if group_by_pc_types:\n",
    "        df = pd.read_csv(PROCESSED_DATA_DIR / f\"multi_{horizon}m_grouped.csv\")\n",
    "    else:\n",
    "        df = pd.read_csv(PROCESSED_DATA_DIR / f\"multi_{horizon}m.csv\")\n",
    "\n",
    "    # Separate features and target\n",
    "    target_col = processed_names.LONG_PC_PRICE\n",
    "    meta_cols = [\n",
    "        processed_names.LONG_DATE,\n",
    "        processed_names.LONG_REGION,\n",
    "        processed_names.LONG_PC_TYPE,\n",
    "    ]\n",
    "    feature_cols = [col for col in df.columns if col not in meta_cols + [target_col]]\n",
    "    logger.info(f\"Data loaded with {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "    logger.info(f\"Target: {target_col}. Features: {len(feature_cols)} columns.\")\n",
    "\n",
    "    return df, target_col, feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, target_col, feature_cols = load_and_prepare_data(group_by_pc_types=True, horizon=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# 2. Split Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "For data not grouped by PC types, we can use a standard train-validation-test split. However, since the data is imbalanced across different pc types, we need to ensure that the train set and the test set contain enough samples from each pc type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use for dataset not grouped by PC types\n",
    "def adaptive_train_validation_test_split(\n",
    "    df: pd.DataFrame,\n",
    "    group_col: str = processed_names.LONG_PC_TYPE,\n",
    "    target_test_ratio: float = 0.1,\n",
    "    target_validation_ratio: float = 0.1,\n",
    "    min_train_samples: int = 20,\n",
    "    min_validation_samples: int = 5,\n",
    "    min_test_samples: int = 5,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Adaptive temporal split that ensures minimum samples per group.\n",
    "\n",
    "    If standard 80/20 split results in insufficient test samples for any group,\n",
    "    adjusts split date to ensure minimum requirements.\n",
    "\n",
    "    Args:\n",
    "        df: Long format dataframe with 'date' column\n",
    "        group_col: Column defining groups\n",
    "        target_test_ratio: Desired test set size (default 0.1 = 10% of total data)\n",
    "        target_validation_ratio: Desired validation set size (default 0.1 = 10%\n",
    "                                 of total data)\n",
    "        min_train_samples: Minimum training samples per group\n",
    "        min_test_samples: Minimum test samples per group\n",
    "        min_validation_samples: Minimum validation samples per group\n",
    "\n",
    "    Returns:\n",
    "        (train_df, validation_df,test_df) with sufficient samples per group\n",
    "    \"\"\"\n",
    "    df = df.sort_values(processed_names.LONG_DATE).reset_index(drop=True)\n",
    "\n",
    "    # Find the latest date that satisfies constraints\n",
    "    dates = sorted(df[processed_names.LONG_DATE].unique())\n",
    "\n",
    "    # Target PC types to check\n",
    "    target_pc_types = [cst.REGULAR_PC_TYPE, cst.GREEN_PC_TYPE]\n",
    "\n",
    "    for validation_split_date in reversed(dates):\n",
    "        possible_test_dates = [date for date in dates if date > validation_split_date]\n",
    "        for test_split_date in reversed(possible_test_dates):\n",
    "            train = df[df[processed_names.LONG_DATE] < validation_split_date]\n",
    "            validation = df[\n",
    "                (df[processed_names.LONG_DATE] >= validation_split_date)\n",
    "                & (df[processed_names.LONG_DATE] < test_split_date)\n",
    "            ]\n",
    "            test = df[df[processed_names.LONG_DATE] >= test_split_date]\n",
    "            # Check if all groups meet minimum requirements\n",
    "            valid = True\n",
    "            for target_pc in target_pc_types:\n",
    "                train_count = (train[group_col] == target_pc).sum()\n",
    "                validation_count = (validation[group_col] == target_pc).sum()\n",
    "                test_count = (test[group_col] == target_pc).sum()\n",
    "\n",
    "                if (\n",
    "                    train_count < min_train_samples\n",
    "                    or validation_count < min_validation_samples\n",
    "                    or test_count < min_test_samples\n",
    "                ):\n",
    "                    valid = False\n",
    "                    break\n",
    "\n",
    "            if valid:\n",
    "                logger.info(\n",
    "                    f\"Found valid split at dates: \"\n",
    "                    f\"validation {validation_split_date}, test {test_split_date}\"\n",
    "                )\n",
    "                actual_test_ratio = len(test) / len(df)\n",
    "                actual_validation_ratio = len(validation) / len(df)\n",
    "                logger.info(f\"Actual test set ratio: {actual_test_ratio:.2%}\")\n",
    "                logger.info(f\"Desired test set ratio: {target_test_ratio:.2%}\")\n",
    "                logger.info(\n",
    "                    f\"Actual validation set ratio: {actual_validation_ratio:.2%}\"\n",
    "                )\n",
    "                logger.info(\n",
    "                    f\"Desired validation set ratio: {target_validation_ratio:.2%}\"\n",
    "                )\n",
    "\n",
    "                return train, validation, test\n",
    "\n",
    "    # If no valid split found, fall back to target ratios\n",
    "    logger.warning(\n",
    "        f\"No valid adaptive split found satisfying minimum sample constraints \"\n",
    "        f\"(train≥{min_train_samples}, validation≥{min_validation_samples}, \"\n",
    "        f\"test≥{min_test_samples}). Falling back to \"\n",
    "        f\"{1 - target_test_ratio - target_validation_ratio:.0%}/\"\n",
    "        f\"{target_validation_ratio:.0%}/{target_test_ratio:.0%} split.\"\n",
    "    )\n",
    "\n",
    "    # Use index-based splitting on unique dates (more reliable than quantile)\n",
    "    dates = sorted(df[processed_names.LONG_DATE].unique())\n",
    "    n_dates = len(dates)\n",
    "\n",
    "    val_idx = int(n_dates * (1 - target_test_ratio - target_validation_ratio))\n",
    "    test_idx = int(n_dates * (1 - target_test_ratio))\n",
    "\n",
    "    split_date_validation = dates[val_idx]\n",
    "    split_date_test = dates[test_idx]\n",
    "\n",
    "    train = df[df[processed_names.LONG_DATE] < split_date_validation]\n",
    "    validation = df[\n",
    "        (df[processed_names.LONG_DATE] >= split_date_validation)\n",
    "        & (df[processed_names.LONG_DATE] < split_date_test)\n",
    "    ]\n",
    "    test = df[df[processed_names.LONG_DATE] >= split_date_test]\n",
    "\n",
    "    # Log warning about groups that don't meet requirements\n",
    "    for target_pc in target_pc_types:\n",
    "        train_count = (train[group_col] == target_pc).sum()\n",
    "        val_count = (validation[group_col] == target_pc).sum()\n",
    "        test_count = (test[group_col] == target_pc).sum()\n",
    "\n",
    "        if (\n",
    "            train_count < min_train_samples\n",
    "            or val_count < min_validation_samples\n",
    "            or test_count < min_test_samples\n",
    "        ):\n",
    "            logger.warning(\n",
    "                f\"PC type '{target_pc}' has insufficient samples in fallback split: \"\n",
    "                f\"train={train_count} (min={min_train_samples}), \"\n",
    "                f\"val={val_count} (min={min_validation_samples}), \"\n",
    "                f\"test={test_count} (min={min_test_samples})\"\n",
    "            )\n",
    "\n",
    "    return train, validation, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, validation, test = adaptive_train_validation_test_split(\n",
    "#     df=df, group_col=processed_names.LONG_PC_TYPE\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "For data grouped by PC types, because the data is imbalanced across different pc types, we only perform a train-test split (no validation set). We need to ensure that the train set and the test set contain enough samples from each pc type. This is a problem especially for rare pc types (`gf20` notably). To do this, we use a function that performs an adaptive train-test split, ensuring that each pc type is represented in both sets with a minimum number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use for data grouped by PC types\n",
    "def adaptive_train_test_split(\n",
    "    df: pd.DataFrame,\n",
    "    group_col: str = processed_names.LONG_PC_TYPE,\n",
    "    target_test_ratio: float = 0.2,\n",
    "    min_train_samples: int = 20,\n",
    "    min_test_samples: int = 5,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Adaptive temporal split that ensures minimum samples per group.\n",
    "\n",
    "    If standard 80/20 split results in insufficient test samples for any group,\n",
    "    adjusts split date to ensure minimum requirements.\n",
    "\n",
    "    Args:\n",
    "        df: Long format dataframe with 'date' column\n",
    "        group_col: Column defining groups\n",
    "        target_test_ratio: Desired test set size (default 0.2 = 20%)\n",
    "        min_train_samples: Minimum training samples per group\n",
    "        min_test_samples: Minimum test samples per group\n",
    "\n",
    "    Returns:\n",
    "        (train_df, test_df) with sufficient samples per group\n",
    "    \"\"\"\n",
    "    df = df.sort_values(processed_names.LONG_DATE).reset_index(drop=True)\n",
    "\n",
    "    # Find the latest date that satisfies constraints\n",
    "    dates = sorted(df[processed_names.LONG_DATE].unique())\n",
    "\n",
    "    for split_date in reversed(dates):\n",
    "        train = df[df[processed_names.LONG_DATE] < split_date]\n",
    "        test = df[df[processed_names.LONG_DATE] >= split_date]\n",
    "        # Check if all groups meet minimum requirements\n",
    "        valid = True\n",
    "        target_pc_types = [\n",
    "            pc_type\n",
    "            for pc_type in df[group_col].unique()\n",
    "            if pc_type in cst.PCType._value2member_map_\n",
    "        ]\n",
    "        for target_pc in target_pc_types:\n",
    "            train_count = (train[group_col] == target_pc).sum()\n",
    "            test_count = (test[group_col] == target_pc).sum()\n",
    "\n",
    "            if train_count < min_train_samples or test_count < min_test_samples:\n",
    "                valid = False\n",
    "                break\n",
    "\n",
    "        if valid:\n",
    "            logger.info(f\"Found valid split at date: {split_date}\")\n",
    "            actual_test_ratio = len(test) / len(df)\n",
    "            logger.info(f\"Actual test set ratio: {actual_test_ratio:.2%}\")\n",
    "            logger.info(f\"Desired test set ratio: {target_test_ratio:.2%}\")\n",
    "\n",
    "            return train, test\n",
    "\n",
    "    # If no valid split found, warn user\n",
    "    logger.warning(\n",
    "        \"Could not find a split date satisfying minimum sample requirements \"\n",
    "        \"for all groups. Falling back to target test ratio split.\"\n",
    "    )\n",
    "\n",
    "    # Fallback: use target ratio\n",
    "    n_test = int(len(dates) * target_test_ratio)\n",
    "    split_date = dates[-n_test]\n",
    "    train = df[df[processed_names.LONG_DATE] < split_date]\n",
    "    test = df[df[processed_names.LONG_DATE] >= split_date]\n",
    "\n",
    "    # Log warning about groups that don't meet requirements\n",
    "    for target_pc in target_pc_types:\n",
    "        train_count = (train[group_col] == target_pc).sum()\n",
    "        test_count = (test[group_col] == target_pc).sum()\n",
    "\n",
    "        if train_count < min_train_samples or test_count < min_test_samples:\n",
    "            logger.warning(\n",
    "                f\"PC type '{target_pc}' has insufficient samples in fallback split: \"\n",
    "                f\"train={train_count} (min={min_train_samples}), \"\n",
    "                f\"test={test_count} (min={min_test_samples})\"\n",
    "            )\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test = adaptive_train_test_split(\n",
    "#     df=df,\n",
    "#     group_col=processed_names.LONG_PC_TYPE,\n",
    "#     min_train_samples=19,\n",
    "#     min_test_samples=5,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# 3. Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(\n",
    "    train_df: pd.DataFrame,\n",
    "    validation_df: pd.DataFrame | None,\n",
    "    test_df: pd.DataFrame,\n",
    "    feature_cols: list[str],\n",
    "    target_col: str,\n",
    "    horizon: int,\n",
    ") -> tuple[\n",
    "    np.ndarray,\n",
    "    np.ndarray | None,\n",
    "    np.ndarray,\n",
    "    np.ndarray,\n",
    "    np.ndarray | None,\n",
    "    np.ndarray,\n",
    "    pd.DataFrame,\n",
    "    pd.DataFrame | None,\n",
    "    pd.DataFrame,\n",
    "]:\n",
    "    \"\"\"Prepare features and target variable for training.\n",
    "\n",
    "    To align the target variable with the features, the target is shifted\n",
    "    by -horizon months within each group defined by region and pc_type.\n",
    "\n",
    "    Args:\n",
    "        train_df (pd.DataFrame): Training dataframe.\n",
    "        validation_df (pd.DataFrame | None): Validation dataframe.\n",
    "        test_df (pd.DataFrame): Testing dataframe.\n",
    "        feature_cols (list[str]): List of feature column names.\n",
    "        target_col (str): Target column name.\n",
    "        horizon (int): Forecast horizon in months.\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.ndarray, np.ndarray | None, np.ndarray, np.ndarray, np.ndarray | None,\n",
    "        np.ndarray, pd.DataFrame, pd.DataFrame | None, pd.DataFrame]:\n",
    "            X_train, y_train, X_validation, y_validation, X_test, y_test,\n",
    "            aligned training, validation and testing dataframes\n",
    "    \"\"\"\n",
    "    # Target (shift by -horizon to align with features)\n",
    "    # Group by region and pc_type to shift correctly\n",
    "    train_df[\"target\"] = train_df.groupby(\n",
    "        [processed_names.LONG_REGION, processed_names.LONG_PC_TYPE]\n",
    "    )[target_col].shift(-horizon)\n",
    "    if validation_df is not None:\n",
    "        validation_df[\"target\"] = validation_df.groupby(\n",
    "            [processed_names.LONG_REGION, processed_names.LONG_PC_TYPE]\n",
    "        )[target_col].shift(-horizon)\n",
    "    test_df[\"target\"] = test_df.groupby(\n",
    "        [processed_names.LONG_REGION, processed_names.LONG_PC_TYPE]\n",
    "    )[target_col].shift(-horizon)\n",
    "\n",
    "    # Drop rows with NaN in target (due to shifting)\n",
    "    train_mask = ~train_df[\"target\"].isna()\n",
    "    if validation_df is not None:\n",
    "        validation_mask = ~validation_df[\"target\"].isna()\n",
    "    test_mask = ~test_df[\"target\"].isna()\n",
    "\n",
    "    X_train = train_df.loc[train_mask, feature_cols].values\n",
    "    y_train = train_df.loc[train_mask, \"target\"].values\n",
    "    train_df_aligned = train_df[train_mask].copy()\n",
    "    logger.info(f\"Training data prepared with {X_train.shape[0]} samples.\")\n",
    "\n",
    "    if validation_df is not None:\n",
    "        X_validation = validation_df.loc[validation_mask, feature_cols].values\n",
    "        y_validation = validation_df.loc[validation_mask, \"target\"].values\n",
    "        validation_df_aligned = validation_df[validation_mask].copy()\n",
    "        logger.info(f\"Validation data prepared with {X_validation.shape[0]} samples.\")\n",
    "\n",
    "    X_test = test_df.loc[test_mask, feature_cols].values\n",
    "    y_test = test_df.loc[test_mask, \"target\"].values\n",
    "    test_df_aligned = test_df[test_mask].copy()\n",
    "    logger.info(f\"Testing data prepared with {X_test.shape[0]} samples.\")\n",
    "\n",
    "    if validation_df is not None:\n",
    "        return (\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_validation,\n",
    "            y_validation,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            train_df_aligned,\n",
    "            validation_df_aligned,\n",
    "            test_df_aligned,\n",
    "        )\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, train_df_aligned, test_df_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train, X_test, y_test, train_df_aligned, test_df_aligned = (\n",
    "#     prepare_training_data(\n",
    "#         train_df=train,\n",
    "#         validation_df=None,\n",
    "#         test_df=test,\n",
    "#         feature_cols=feature_cols,\n",
    "#         target_col=target_col,\n",
    "#         horizon=3,\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# (\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     X_validation,\n",
    "#     y_validation,\n",
    "#     X_test,\n",
    "#     y_test,\n",
    "#     train_df_aligned,\n",
    "#     validation_df_aligned,\n",
    "#     test_df_aligned,\n",
    "# ) = prepare_training_data(\n",
    "#     train_df=train,\n",
    "#     validation_df=validation,\n",
    "#     test_df=test,\n",
    "#     feature_cols=feature_cols,\n",
    "#     target_col=target_col,\n",
    "#     horizon=3,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# 4. Compute Sample Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Because the data is imbalanced across different pc types, we compute sample weights to give more importance to under-represented pc types during model training. This helps the model to learn better representations for these rare pc types. Without this, the model might be biased towards the more common pc types, leading to poor performance on the rare ones. The global performance metric might be good, but the performance on rare pc types would be bad.\n",
    "\n",
    "Additionally, we can also weight samples based on region: we are only concerned about performance in Europe, so we can give more weight to samples from this region. We keep pc types from all regions in the training set to have more data, but we want to prioritize performance on European pc types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sample_weights(\n",
    "    df: pd.DataFrame,\n",
    "    group_col: str,\n",
    "    region_col: str,\n",
    "    target_region: str,\n",
    "    method: Literal[\"inverse_frequency\", \"sqrt_inverse\", \"balanced\"] = \"balanced\",\n",
    "    region_weight_multiplier: float = 1.5,\n",
    ") -> pd.Series:\n",
    "    \"\"\"Compute sample weights based on group frequency and region.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataframe containing the data.\n",
    "        group_col (str): Column name for grouping (e.g., pc_type).\n",
    "        region_col (str): Column name for region.\n",
    "        target_region (str): Region to prioritize.\n",
    "        method (Literal[\"inverse_frequency\", \"sqrt_inverse\", \"balanced\"], optional):\n",
    "            Method to compute weights. Defaults to \"balanced\".\n",
    "        region_weight_multiplier (float, optional): Multiplier for weights of the\n",
    "            target region. Defaults to 1.5.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Sample weights for each row in the dataframe.\n",
    "    \"\"\"\n",
    "    # Validate method\n",
    "    if method not in [\"inverse_frequency\", \"sqrt_inverse\", \"balanced\"]:\n",
    "        raise ValueError(\n",
    "            f\"Invalid method: {method}. Choose from 'inverse_frequency', \"\n",
    "            \"'sqrt_inverse', 'balanced'.\"\n",
    "        )\n",
    "\n",
    "    group_counts = df[group_col].value_counts()\n",
    "    n_samples = len(df)\n",
    "    n_groups = len(group_counts)\n",
    "\n",
    "    # Simple inverse frequency weights (1/count)\n",
    "    if method == \"inverse_frequency\":\n",
    "        weights_map = {group: 1.0 / count for group, count in group_counts.items()}\n",
    "        weights = df[group_col].map(weights_map).values\n",
    "\n",
    "    # Square root of inverse frequency weights\n",
    "    # Less aggressive than inverse_frequency\n",
    "    elif method == \"sqrt_inverse\":\n",
    "        weights_map = {\n",
    "            group: 1.0 / np.sqrt(count) for group, count in group_counts.items()\n",
    "        }\n",
    "        weights = df[group_col].map(weights_map).values\n",
    "\n",
    "    elif method == \"balanced\":\n",
    "        # Sklearn-style balanced weights: n_samples / (n_groups * count)\n",
    "        # Normalized so sum(weights) ≈ n_samples (maintains effective sample size)\n",
    "        weights_map = {\n",
    "            group: n_samples / (n_groups * count)\n",
    "            for group, count in group_counts.items()\n",
    "        }\n",
    "        weights = df[group_col].map(weights_map).values\n",
    "\n",
    "        # Apply region multiplier\n",
    "    region_multiplier = np.where(\n",
    "        df[region_col] == target_region,\n",
    "        region_weight_multiplier,  # 1.5× weight for Europe\n",
    "        1.0,  # 1.0× weight for Asia\n",
    "    )\n",
    "    weights = weights * region_multiplier\n",
    "\n",
    "    # Renormalize to maintain effective sample size\n",
    "    weights = weights * len(df) / weights.sum()\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "# 5. Define evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "For model evaluation, we will use the Mean Absolute Percentage Error (MAPE) as our primary metric. MAPE is particularly useful in this context because it provides a normalized measure of prediction accuracy, allowing us to assess how well our model performs across different pc types and price ranges. However, using just a global MAPE can be misleading due to the imbalanced nature of the dataset. To address this, we will also compute a weighted MAPE, where each pc type's contribution to the overall metric is weighted inversely proportional to its frequency in the dataset. This approach ensures that the model's performance on rare pc types is adequately represented in the evaluation, preventing the model from being overly optimized for the more common pc types at the expense of the rare ones. We have $3$ metrics in total:\n",
    "- Global MAPE: Overall MAPE across all samples.\n",
    "- Weighted MAPE: MAPE computed with pc type weights.\n",
    "- Per pc type MAPE: MAPE computed for each pc type separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_performance_metrics(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    pc_types: pd.Series,\n",
    "    weights: np.ndarray = None,\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"Compute performance metrics for the model.\n",
    "\n",
    "    Three performance metrics are computed:\n",
    "    - Global MAPE: Mean Absolute Percentage Error across all samples.\n",
    "    - Weighted MAPE: MAPE computed with sample weights.\n",
    "    - Per pc type MAPE: MAPE computed for each pc type separately.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): True target values.\n",
    "        y_pred (np.ndarray): Predicted target values.\n",
    "        pc_types (pd.Series): PC types of the true target values.\n",
    "        weights (np.ndarray, optional): Sample weights. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, float]: Dictionary containing the computed performance metrics.\n",
    "    \"\"\"\n",
    "    performance_metrics = {}\n",
    "\n",
    "    # Global MAPE\n",
    "    global_mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    performance_metrics[cst.GLOBAL_MAPE] = global_mape\n",
    "\n",
    "    # Weighted MAPE\n",
    "    if weights is not None:\n",
    "        weighted_mape = mean_absolute_percentage_error(\n",
    "            y_true, y_pred, sample_weight=weights\n",
    "        )\n",
    "        performance_metrics[cst.WEIGHTED_MAPE] = weighted_mape\n",
    "\n",
    "    # Per PC type MAPE\n",
    "    for pc_type in pc_types.unique():\n",
    "        pc_type_mask = pc_types == pc_type\n",
    "        y_pc_specific_true = y_true[pc_type_mask]\n",
    "        y_pc_specific_pred = y_pred[pc_type_mask]\n",
    "        pc_specific_mape = mean_absolute_percentage_error(\n",
    "            y_pc_specific_true, y_pc_specific_pred\n",
    "        )\n",
    "        performance_metrics[f\"{pc_type}_MAPE\"] = pc_specific_mape\n",
    "\n",
    "    return performance_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# 6. Train Global Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "We train a single global model on all pc types using the computed sample weights, and log it to MLflow. This gives us the following training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_global_model(\n",
    "    group_by_pc_types: bool,\n",
    "    horizon: int,\n",
    "    target_test_ratio: float,\n",
    "    target_validation_ratio: float,\n",
    "    min_train_samples: int,\n",
    "    min_validation_samples: int,\n",
    "    min_test_samples: int,\n",
    "    weighting_method: Literal[\"inverse_frequency\", \"sqrt_inverse\", \"balanced\"],\n",
    "    model_type: Literal[\"xgboost\", \"random_forest\", \"lightgbm\", \"catboost\", \"tft\"],\n",
    "    hyperparameter_grid: dict | None,\n",
    "    mlflow_run_name: str,\n",
    "    n_trials: int,\n",
    ") -> None:\n",
    "    \"\"\"Train a global model on all PC types and log it to MLflow.\n",
    "\n",
    "    Args:\n",
    "        group_by_pc_types (bool): Whether PC prices are grouped by type.\n",
    "        horizon (int): Forecast horizon in months.\n",
    "        target_test_ratio (float): Desired test set size.\n",
    "        target_validation_ratio (float): Desired validation set size.\n",
    "        min_train_samples (int): Minimum training samples per group.\n",
    "        min_validation_samples (int): Minimum validation samples per group.\n",
    "        min_test_samples (int): Minimum test samples per group.\n",
    "        weighting_method (Literal): Method to compute sample weights.\n",
    "        model_type (Literal): Type of model to train.\n",
    "        hyperparameter_grid (dict | None): Hyperparameter grid for optimization.\n",
    "        mlflow_run_name (str): Name for the MLflow run.\n",
    "        n_trials (int): Number of trials for hyperparameter optimization.\n",
    "    \"\"\"\n",
    "    # 1. Load and prepare data\n",
    "    df, target_col, feature_cols = load_and_prepare_data(\n",
    "        group_by_pc_types=group_by_pc_types, horizon=horizon\n",
    "    )\n",
    "\n",
    "    # 2. Split data into training and testing sets\n",
    "    if group_by_pc_types:\n",
    "        train_df, test_df = adaptive_train_test_split(\n",
    "            df=df,\n",
    "            group_col=processed_names.LONG_PC_TYPE,\n",
    "            target_test_ratio=target_test_ratio,\n",
    "            min_train_samples=min_train_samples,\n",
    "            min_test_samples=min_test_samples,\n",
    "        )\n",
    "    else:\n",
    "        train_df, validation_df, test_df = adaptive_train_validation_test_split(\n",
    "            df=df,\n",
    "            group_col=processed_names.LONG_PC_TYPE,\n",
    "            target_test_ratio=target_test_ratio,\n",
    "            target_validation_ratio=target_validation_ratio,\n",
    "            min_train_samples=min_train_samples,\n",
    "            min_validation_samples=min_validation_samples,\n",
    "            min_test_samples=min_test_samples,\n",
    "        )\n",
    "\n",
    "    # 3. Prepare training and testing data\n",
    "    if group_by_pc_types:\n",
    "        X_train, y_train, X_test, y_test, train_df_aligned, test_df_aligned = (\n",
    "            prepare_training_data(\n",
    "                train_df=train_df,\n",
    "                validation_df=None,\n",
    "                test_df=test_df,\n",
    "                feature_cols=feature_cols,\n",
    "                target_col=target_col,\n",
    "                horizon=horizon,\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        (\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_validation,\n",
    "            y_validation,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            train_df_aligned,\n",
    "            validation_df_aligned,\n",
    "            test_df_aligned,\n",
    "        ) = prepare_training_data(\n",
    "            train_df=train_df,\n",
    "            validation_df=validation_df,\n",
    "            test_df=test_df,\n",
    "            feature_cols=feature_cols,\n",
    "            target_col=target_col,\n",
    "            horizon=horizon,\n",
    "        )\n",
    "\n",
    "    # 4. Compute sample weights for train and test\n",
    "    train_sample_weights = compute_sample_weights(\n",
    "        df=train_df_aligned,\n",
    "        group_col=processed_names.LONG_PC_TYPE,\n",
    "        region_col=processed_names.LONG_REGION,\n",
    "        target_region=cst.EUROPE,\n",
    "        method=weighting_method,\n",
    "    )\n",
    "\n",
    "    if not group_by_pc_types:\n",
    "        validation_sample_weights = compute_sample_weights(\n",
    "            df=validation_df_aligned,\n",
    "            group_col=processed_names.LONG_PC_TYPE,\n",
    "            region_col=processed_names.LONG_REGION,\n",
    "            target_region=cst.EUROPE,\n",
    "            method=weighting_method,\n",
    "        )\n",
    "\n",
    "    test_sample_weights = compute_sample_weights(\n",
    "        df=test_df_aligned,\n",
    "        group_col=processed_names.LONG_PC_TYPE,\n",
    "        region_col=processed_names.LONG_REGION,\n",
    "        target_region=cst.EUROPE,\n",
    "        method=weighting_method,\n",
    "    )\n",
    "\n",
    "    # 5. Train model\n",
    "    if model_type == \"xgboost\":\n",
    "        if group_by_pc_types:\n",
    "            best_params = optimize_xgboost_model(\n",
    "                X_train=X_train,\n",
    "                y_train=y_train,\n",
    "                X_test=X_test,\n",
    "                y_test=y_test,\n",
    "                weights=test_sample_weights,\n",
    "                hyperparameter_grid=hyperparameter_grid,\n",
    "                pc_types=test_df_aligned[processed_names.LONG_PC_TYPE],\n",
    "                n_trials=n_trials,\n",
    "            )\n",
    "        else:\n",
    "            best_params = optimize_xgboost_model(\n",
    "                X_train=X_train,\n",
    "                y_train=y_train,\n",
    "                X_test=X_validation,\n",
    "                y_test=y_validation,\n",
    "                weights=validation_sample_weights,\n",
    "                hyperparameter_grid=hyperparameter_grid,\n",
    "                pc_types=validation_df_aligned[processed_names.LONG_PC_TYPE],\n",
    "                n_trials=n_trials,\n",
    "            )\n",
    "        eval_model = XGBRegressor(**best_params)\n",
    "        pred_model = XGBRegressor(**best_params)\n",
    "\n",
    "        # 6. Evaluate and log model\n",
    "        evaluate_and_log_model(\n",
    "            eval_model=eval_model,\n",
    "            pred_model=pred_model,\n",
    "            best_params=best_params,\n",
    "            mlflow_run_name=mlflow_run_name,\n",
    "            model_type=model_type,\n",
    "            horizon=horizon,\n",
    "            group_by_pc_types=group_by_pc_types,\n",
    "            weighting_method=weighting_method,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_validation=X_validation if not group_by_pc_types else None,\n",
    "            y_validation=y_validation if not group_by_pc_types else None,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            train_df_aligned=train_df_aligned,\n",
    "            validation_df_aligned=validation_df_aligned\n",
    "            if not group_by_pc_types\n",
    "            else None,\n",
    "            test_df_aligned=test_df_aligned,\n",
    "            train_sample_weights=train_sample_weights,\n",
    "            test_sample_weights=test_sample_weights,\n",
    "        )\n",
    "\n",
    "    elif model_type == \"random_forest\":\n",
    "        # Placeholder for random forest implementation\n",
    "        pass\n",
    "\n",
    "    elif model_type == \"lightgbm\":\n",
    "        # Placeholder for LightGBM implementation\n",
    "        pass\n",
    "\n",
    "    elif model_type == \"catboost\":\n",
    "        # Placeholder for CatBoost implementation\n",
    "        pass\n",
    "\n",
    "    elif model_type == \"tft\":\n",
    "        # Placeholder for TFT implementation\n",
    "        pass\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Model type '{model_type}' not implemented.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_global_model(\n",
    "    group_by_pc_types=False,\n",
    "    horizon=3,\n",
    "    target_test_ratio=0.2,\n",
    "    target_validation_ratio=0.1,\n",
    "    min_train_samples=19,\n",
    "    min_test_samples=50,\n",
    "    min_validation_samples=50,\n",
    "    weighting_method=\"balanced\",\n",
    "    model_type=\"xgboost\",\n",
    "    hyperparameter_grid=None,\n",
    "    mlflow_run_name=\"global_xgboost_model_3m_horizon_no_grouping_v3\",\n",
    "    n_trials=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.search_runs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "14-data-challenge (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
